<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Darius Goergen" />


<title>Convolutional Neural Network</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotly-main-1.46.1/plotly-latest.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">polymeRID</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="preparation.html">Preparation</a>
</li>
<li>
  <a href="exploration.html">Exploration</a>
</li>
<li>
  <a href="calibration.html">Calibration</a>
</li>
<li>
  <a href="classification.html">Classification</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Convolutional Neural Network</h1>
<h4 class="author">Darius Goergen</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2019-08-22
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>polymeRID/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.4.0.9001). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20190729code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20190729)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20190729code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20190729)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomgoergen95polymeRIDtree7e9eddd2a668f3ff384f2643cd2edd320639533etargetblank7e9eddda"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/goergen95/polymeRID/tree/7e9eddd2a668f3ff384f2643cd2edd320639533e" target="_blank">7e9eddd</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomgoergen95polymeRIDtree7e9eddd2a668f3ff384f2643cd2edd320639533etargetblank7e9eddda" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    .Rprofile
    Ignored:    .Rproj.user/
    Ignored:    analysis/library.bib
    Ignored:    docs/figure/
    Ignored:    fun/
    Ignored:    output/20190810_1538/
    Ignored:    output/20190810_1546/
    Ignored:    output/20190810_1609/
    Ignored:    output/20190813_1044/
    Ignored:    output/logs/
    Ignored:    output/natural/
    Ignored:    output/nnet/
    Ignored:    output/svm/
    Ignored:    output/testRunII/
    Ignored:    output/testRunIII/
    Ignored:    packrat/lib-R/
    Ignored:    packrat/lib-ext/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/BH/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/FactoMineR/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/IDPmisc/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/KernSmooth/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/MASS/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/Matrix/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/MatrixModels/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ModelMetrics/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/R6/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/RColorBrewer/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/RCurl/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/Rcpp/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/RcppArmadillo/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/RcppEigen/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/RcppGSL/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/RcppZiggurat/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/Rfast/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/Rgtsvm/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/Rmisc/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/SQUAREM/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/SparseM/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/abind/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/askpass/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/assertthat/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/backports/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/base64enc/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/baseline/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/bit/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/bit64/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/bitops/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/boot/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/callr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/car/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/carData/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/caret/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/cellranger/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/class/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/cli/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/clipr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/cluster/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/codetools/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/colorspace/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/config/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/cowplot/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/crayon/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/crosstalk/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/curl/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/data.table/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/dendextend/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/digest/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/doParallel/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/dplyr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/e1071/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ellipse/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ellipsis/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/evaluate/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/factoextra/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/fansi/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/flashClust/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/forcats/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/foreach/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/foreign/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/fs/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/generics/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/getPass/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ggplot2/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ggpubr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ggrepel/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ggsci/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ggsignif/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/git2r/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/glue/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/gower/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/gridExtra/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/gtable/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/haven/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/hexbin/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/highr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/hms/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/htmltools/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/htmlwidgets/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/httpuv/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/httr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ipred/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/iterators/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/jsonlite/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/keras/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/kerasR/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/knitr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/labeling/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/later/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/lattice/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/lava/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/lazyeval/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/leaps/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/lme4/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/lubridate/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/magrittr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/maptools/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/markdown/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/mgcv/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/mime/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/minqa/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/munsell/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/nlme/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/nloptr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/nnet/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/numDeriv/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/openssl/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/openxlsx/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/packrat/tests/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/pbkrtest/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/pillar/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/pkgconfig/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/plogr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/plotly/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/plyr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/polynom/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/prettyunits/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/processx/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/prodlim/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/progress/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/promises/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/prospectr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/ps/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/purrr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/quantreg/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/randomForest/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/readr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/readxl/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/recipes/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rematch/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/reshape2/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/reticulate/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rio/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rlang/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rmarkdown/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rpart/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rprojroot/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rsconnect/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/rstudioapi/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/scales/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/scatterplot3d/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/shiny/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/sourcetools/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/sp/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/stringi/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/stringr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/survival/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/sys/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/tensorflow/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/tfruns/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/tibble/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/tidyr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/tidyselect/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/timeDate/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/tinytex/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/utf8/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/vctrs/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/viridis/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/viridisLite/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/whisker/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/withr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/workflowr/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/xfun/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/xtable/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/yaml/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/zeallot/
    Ignored:    packrat/lib/x86_64-pc-linux-gnu/3.6.1/zip/
    Ignored:    packrat/src/
    Ignored:    polymeRID.Rproj
    Ignored:    smp/20190812_1723_NNET/files/
    Ignored:    smp/20190812_1723_NNET/plots/
    Ignored:    smp/20190812_1729_NNET/files/
    Ignored:    smp/20190812_1729_NNET/plots/
    Ignored:    smp/20190812_1731_NNET/files/
    Ignored:    smp/20190812_1731_NNET/plots/
    Ignored:    smp/20190812_1733_NNET/files/
    Ignored:    smp/20190812_1733_NNET/plots/
    Ignored:    smp/20190815_1847_FUSION/
    Ignored:    website/

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/goergen95/polymeRID/blob/7e9eddd2a668f3ff384f2643cd2edd320639533e/analysis/cnn_exploration.Rmd" target="_blank">7e9eddd</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-22
</td>
<td>
wflow_publish(files = c(“analysis/cnn_crossvalidation.Rmd”, “analysis/cnn_exploration.Rmd”,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/goergen95/polymeRID/f2ee83c6fd5264329023a1ff1bccf40d5fa9dee4/docs/cnn_exploration.html" target="_blank">f2ee83c</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-19
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/goergen95/polymeRID/d960dc216c32dcaf53e01d4d78272315f2e03c95/docs/cnn_exploration.html" target="_blank">d960dc2</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-19
</td>
<td>
included calibration
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/goergen95/polymeRID/b846f0b3c5c84ad163a88d86431d119da18d8cef/docs/cnn_exploration.html" target="_blank">b846f0b</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-19
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/goergen95/polymeRID/blob/de84a71f5f9d7350d8143dc015520f2fc65303e8/analysis/cnn_exploration.Rmd" target="_blank">de84a71</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-19
</td>
<td>
large update for website
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/goergen95/polymeRID/de84a71f5f9d7350d8143dc015520f2fc65303e8/docs/cnn_exploration.html" target="_blank">de84a71</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-19
</td>
<td>
large update for website
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/goergen95/polymeRID/blob/c0b21db62eefed4562234b7eee26331b85d89c52/analysis/cnn_exploration.Rmd" target="_blank">c0b21db</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-15
</td>
<td>
proceeded on CNN exploration
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/goergen95/polymeRID/c0b21db62eefed4562234b7eee26331b85d89c52/docs/cnn_exploration.html" target="_blank">c0b21db</a>
</td>
<td>
goergen95
</td>
<td>
2019-08-15
</td>
<td>
proceeded on CNN exploration
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>Convolutional Neural Networks (CNN) are mainly used in image processing tasks <span class="citation">(Rawat and Wang <a href="#ref-Rawat2017">2017</a>)</span>. However, they can also be applied to one-dimensional data, such as time series or spectral data <span class="citation">(Liu et al. <a href="#ref-Liu2017">2017</a>; Ismail Fawaz et al. <a href="#ref-IsmailFawaz2019">2019</a>; Ghosh et al. <a href="#ref-Ghosh2019">2019</a>; Berisha et al. <a href="#ref-Berisha2019">2019</a>)</span>. They mainly consist of three different types of layers, which generally are stacked into a sequential model for learning various patterns from the input data to model the desired output. The most relevant layer is the convolutional layer, which serves as an extractor for features found in the input <span class="citation">(Rawat and Wang <a href="#ref-Rawat2017">2017</a>)</span>. They work on a specificed number of neurons, or filters, each serving as a mapping function for a specific range in the input data, also referred to as the kernel size. Not only do they map features from the raw input data, but also detect features in the output of previous convolutional layers. This is achieved through adjusting the weights associated with each filter based on a non-linear activation function. Additionally, after some convolutional layers, pooling layers are most commonly included in CNNs. These layers reduce the feature maps of previous layers and are also associated with a function to choose which parameters are preserved. Nowadays, most commonly max-pooling layers are used, preserving the maximum signal from a feature map. Finally, most CNNs end with a fully-connected layer. These layers are used to transform the last feature map to the output. For regression problems, this layer may only contain a single neuron, while for classification problems it may contain <code>n</code>-neurons, each modelling the output of a specific class. The network learns through what is called “backpropagation”. It means that the training data is repeatedly presented to the network, depending on an optimizer function the distance to the desired output is calculated. Then, the weights associated with the filters are updated and a new epoch of presenting the training data to the CNN is started.</p>
</div>
<div id="model-architecture" class="section level2">
<h2>Model Architecture</h2>
<p>In contrast to random forest and support vector machines, the effect of noise on the classification outcome was not tested. This is mainly due to limitations in computation time. The computation time was significantly reduced by installing the <code>keras</code> package in GPU mode based on the <code>CUDA</code> library from <a href="https://developer.nvidia.com/cuda-zone">Nvidia</a>. However, CNNs remain computational intensive, since depending on the architecture several thousands of weights have to be trained. Here, we developed a simple two-block architecture of four convolutional layers in total. The number of filters, or feature extractors, increases with each layer by a factor of 2. We chose this network architechture with four layers and only a small number of filters because it delivered relatively high accuracies in short computation times. The code below defines a function to set up and compile a CNN for a given kernel size.</p>
<pre class="r"><code># expects that you have installed keras and tensorflow properly
library(keras)

buildCNN &lt;- function(kernel, nVariables, nOutcome){
  model = keras_model_sequential()
  model %&gt;%
    # block 1
    layer_conv_1d(filters = 8,
                  kernel_size = kernel,
                  input_shape = c(nVariables,1),
                  name = &quot;block1_conv1&quot;,) %&gt;%
    layer_activation_relu(name=&quot;block1_relu1&quot;) %&gt;%
    layer_conv_1d(filters = 16,
                  kernel_size = kernel,
                  name = &quot;block1_conv2&quot;) %&gt;%
    layer_activation_relu(name=&quot;block1_relu2&quot;) %&gt;%
    layer_max_pooling_1d(strides=2,
                         pool_size = 5,
                         name=&quot;block1_max_pool1&quot;) %&gt;%
    
    # block 2
    layer_conv_1d(filters = 32,
                  kernel_size = kernel,
                  name = &quot;block2_conv1&quot;) %&gt;%
    layer_activation_relu(name=&quot;block2_relu1&quot;) %&gt;%
    layer_conv_1d(filters = 64,
                  kernel_size = kernel,
                  name = &quot;block2_conv2&quot;) %&gt;%
    layer_activation_relu(name=&quot;block2_relu2&quot;) %&gt;%
    layer_max_pooling_1d(strides=2,
                         pool_size = 5,
                         name=&quot;block2_max_pool1&quot;) %&gt;%
    
    # exit block
    layer_global_max_pooling_1d(name=&quot;exit_max_pool&quot;) %&gt;%
    layer_dropout(rate=0.5) %&gt;%
    layer_dense(units = nOutcome, activation = &quot;softmax&quot;)
  
  # we compile for a classification with the categorcial crossentropy loss function
  # and use adam as optimizer function
  compile(model, loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot;)
}</code></pre>
<p>The function expects three arguments as input. The first is the kernel size which specifies the width of the window, extracting features from the input data and subsequent layer outputs. Note that the kernel size is held constant through out the network. The second argument expects an integer representing the number of input variables which relates to the amount of wavenumbers in the present case. The third argument also expects an integer value, specifiying the number of classes in the output. Each convolutional layer is associated with a ReLU-activation function. At the end of each block we added a max-pooling layer with <code>stride = 2</code>, which takes the maximum values of its respective input and discards unrequired data points, effectively reducing the feature space by half. The exit block again consists of a global-max-pooling layer and is followed by a dropout layer which randomly silences half of the neurons to reduce the influence of overfitting. The last layer is a fully-connected layer which maps its input to <code>nOutcome</code> classes via the softmax activation function. The last line of code compiles the model so it is ready for training. We use categorical crossentropy as the loss function in our network because we currently have 14 different classes which perfectly fit for one-hot encoding. If the number of classes is too high, for example in speech recognition problems, sparse categorical crossentropy would be the loss function of choice. As an optimizer function we chose <code>adam</code> because it ensures that the learning rate and decay values will be changed adaptively during training. Finally, we tell the model to optimize the training process based on overall accuracy. We can now compile a first model and take a look at its structure:</p>
<pre class="r"><code>model = buildCNN(kernel = 50, nVariables = 1863, nOutcome = 12)
model</code></pre>
<pre><code>Model
Model: &quot;sequential&quot;
___________________________________________________________________________
Layer (type)                     Output Shape                  Param #     
===========================================================================
block1_conv1 (Conv1D)            (None, 1814, 8)               408         
___________________________________________________________________________
block1_relu1 (ReLU)              (None, 1814, 8)               0           
___________________________________________________________________________
block1_conv2 (Conv1D)            (None, 1765, 16)              6416        
___________________________________________________________________________
block1_relu2 (ReLU)              (None, 1765, 16)              0           
___________________________________________________________________________
block1_max_pool1 (MaxPooling1D)  (None, 881, 16)               0           
___________________________________________________________________________
block2_conv1 (Conv1D)            (None, 832, 32)               25632       
___________________________________________________________________________
block2_relu1 (ReLU)              (None, 832, 32)               0           
___________________________________________________________________________
block2_conv2 (Conv1D)            (None, 783, 64)               102464      
___________________________________________________________________________
block2_relu2 (ReLU)              (None, 783, 64)               0           
___________________________________________________________________________
block2_max_pool1 (MaxPooling1D)  (None, 390, 64)               0           
___________________________________________________________________________
exit_max_pool (GlobalMaxPooling1 (None, 64)                    0           
___________________________________________________________________________
dropout (Dropout)                (None, 64)                    0           
___________________________________________________________________________
dense (Dense)                    (None, 12)                    780         
===========================================================================
Total params: 135,700
Trainable params: 135,700
Non-trainable params: 0
___________________________________________________________________________</code></pre>
<p>In total, the current network consists of 135,700 weights to be trained. In the column <code>output shape</code> we observe the shape transformation of the input data from a 1D-array of 1814 in size on the top layer of the network, to a 1D-output of size 12 on the bottom layer.</p>
</div>
<div id="training-a-cnn" class="section level2">
<h2>Training a CNN</h2>
<p>We can use our database to start a training process with the CNN defined before. First, the input data needs to be transformed to arrays which can be understood by the <code>keras::fit()</code> function. Here, we used <code>keras-backend</code> functionality to achieve this. Additionally, every training process needs to be initiated with information on the number of epochs the training data is going to be presented. We used a fixed value of 300 epochs because beyond that value no substantial gain in accuracy was observed. Also, the training data is going to be presented in batches, each of 10 observations.</p>
<pre class="r"><code>data = read.csv(file = paste0(ref, &quot;reference_database.csv&quot;), header = TRUE)

K &lt;- keras::backend()
x_train = as.matrix(data[,1:ncol(data)-1])
x = K$expand_dims(x_train, axis = 2L)
x_train = K$eval(x)
y_train = keras::to_categorical(as.numeric(data$class)-1, length(unique(data$class)))

history = keras::fit(model, x = x_train, y = y_train,
                               epochs=300,
                               batch_size = 10)
history
plot(history)</code></pre>
<pre><code>Trained on 147 samples (batch_size=10, epochs=300)
Final epoch (plot to see history):
loss: 0.05043
 acc: 0.9796 </code></pre>
<div class="figure" style="text-align: center">
<div id="htmlwidget-5e9d4804ca425d337524" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-5e9d4804ca425d337524">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300],"y":[0.355066610639598,0.374148911359359,0.360360828969552,0.371215659390096,0.546029404920786,0.398030579901066,0.395687515841049,0.331198517055739,0.334919276995724,0.352674266006671,0.339645381788818,0.297134590721657,0.305955297281953,0.314068493490316,0.446525168459432,0.342111969328657,0.347881008725183,0.410880642886065,0.324634762341473,0.324879497593763,0.230280692936206,0.317194948999249,0.309244403248133,0.337695830533294,0.252295367261332,0.27538464350157,0.196764174823453,0.340106711399798,0.246753340979823,0.285608838255308,0.247610494570464,0.30301547590263,0.29170614105909,0.2105263344386,0.216400716580501,0.204093981255479,0.264078910443552,0.258675153183491,0.300135197231964,0.247083302156455,0.228393737100014,0.171263363288373,0.20979498777868,0.272830667013784,0.170032086167611,0.183334239029965,0.33189504464366,0.410154680677113,0.312503496375011,0.153187157725599,0.173914567092244,0.269684011205321,0.377109479944722,0.309988368125189,0.179439492118196,0.183218195571938,0.197238567608352,0.250829318147089,0.255643480520721,0.174638525563843,0.201713891444905,0.150790709197572,0.217829128055751,0.173454684161005,0.243772589392504,0.184426071895223,0.132986239075255,0.173495073403631,0.122822419211243,0.149981741774447,0.385950695267137,0.40973060484342,0.168343291168107,0.191166893914849,0.287912798310522,0.209479963736368,0.303999252465307,0.262471448476458,0.18936352406432,0.160625900236928,0.172082322104802,0.150214163599168,0.103706617668575,0.193591216693119,0.318575792825546,0.137602150541585,0.146193695058223,0.296035569180183,0.214521405572186,0.116490525865078,0.103363903642467,0.147951071283647,0.149707281949366,0.110519534635807,0.294299371787632,0.188613484974722,0.181165060623973,0.132632022216713,0.0966988824011733,0.122313432230735,0.093548818884202,0.113996520319314,0.118892918781577,0.0983704615156261,0.0858093406941716,0.110803271814877,0.112762743453527,0.236584999921675,0.169917962203423,0.208045395515656,0.165660871972083,0.131221944985747,0.152624579640676,0.158253236376003,0.0762373290173583,0.130403010456563,0.107864657220538,0.0779184181889405,0.0728528456394972,0.0952654765256788,0.0788490711236182,0.100806017621693,0.116668565418939,0.161028909674358,0.25641078144616,0.376495750887053,0.303517799190923,0.278046786824723,0.211146742206852,0.1820311545312,0.19712523548376,0.0694770632029771,0.108724717443594,0.102673295847311,0.0934496015836136,0.104593659820808,0.0610873222503127,0.0963302831669167,0.20660979955039,0.142373169335176,0.0938388868821088,0.106242170718936,0.191781994405196,0.148230417061369,0.183803205986564,0.157303344306289,0.0889097800918658,0.0607950647336234,0.126682694769919,0.0894863717004555,0.0958521485124354,0.100814906683858,0.0968429662586589,0.109668837714509,0.173884947641696,0.106593192992162,0.094957384299867,0.113562609914842,0.14088626594289,0.139536568226584,0.0774135947430215,0.128967485618683,0.199622197355441,0.0700126627498135,0.107700231024654,0.152654837552643,0.0895964602027468,0.161249406624674,0.0916936485806391,0.0505824930487568,0.122512205133253,0.094046747871852,0.110685300232437,0.156458068045084,0.0707667532050665,0.0647599385753527,0.0883847014736194,0.0628627557216409,0.121801884128985,0.0501622819012272,0.0669224385020076,0.0901870377421962,0.0600625963526822,0.0810378313375016,0.117217965455738,0.150545926751936,0.11176663261702,0.0995596270310553,0.147648842401841,0.125892719360414,0.103857958964927,0.13843523100538,0.0762564197093958,0.0747746938766201,0.155774415040636,0.084254335916164,0.104902463789307,0.161977186883193,0.105254221169063,0.0837128351539133,0.0464893652404956,0.138955791487175,0.0634328037956539,0.0630925810305072,0.0712856034349118,0.0847201417113154,0.0797892895568365,0.0926923945030993,0.0577676183051413,0.0912480419811509,0.0664595435672195,0.107111122085377,0.118160605165303,0.0737792645577552,0.129020228767875,0.342665791068049,0.131313637108067,0.140155378311752,0.11645476956421,0.201974651682805,0.152753801066048,0.169164528918205,0.0712060089900988,0.0874342441726729,0.0962442551061612,0.255225704336653,0.0781014952143388,0.11683434492522,0.133393186814513,0.114960846744579,0.121477351127965,0.148682134276451,0.0933971100036993,0.106413259139272,0.118792085458531,0.145724406503901,0.264167383718653,0.246101144283098,0.229998317205024,0.133765506768777,0.114190345158053,0.0561086165466897,0.0747683677366194,0.132154763836198,0.069295865148591,0.0713074868168764,0.127930443360134,0.114749593967509,0.211811494614397,0.114400179980665,0.155018048449641,0.109633296500253,0.0714046090253178,0.0758681146785946,0.127281613565378,0.160401479448794,0.0592061673610972,0.0727106089422897,0.0414348421875723,0.095483023069837,0.0653854939375044,0.0512107258970721,0.0890389733300798,0.098915711865716,0.0769481056125923,0.0448047499569152,0.0909682565105788,0.0697121915394696,0.100869539719658,0.0490264130829909,0.069218103625007,0.0560822240313304,0.0719706405518057,0.0665047508664429,0.0666915864687548,0.136988083648236,0.0729775251266325,0.0427120626915158,0.0818181143872452,0.0906775618189008,0.057944027890711,0.0369637997929222,0.0614679835046624,0.0307137383465642,0.0648112535295726,0.0244985639158146,0.0521052218615283,0.0479914024050528,0.0534341581568493,0.065819563977748,0.0705086093142131,0.106105855536223,0.0446256612367444,0.0810714820953744,0.10342975518172,0.0532600103784828,0.059515814984702,0.0551882501942081,0.118192539905748,0.050426438279754],"text":["epoch:   1<br />value: 0.35506661","epoch:   2<br />value: 0.37414891","epoch:   3<br />value: 0.36036083","epoch:   4<br />value: 0.37121566","epoch:   5<br />value: 0.54602940","epoch:   6<br />value: 0.39803058","epoch:   7<br />value: 0.39568752","epoch:   8<br />value: 0.33119852","epoch:   9<br />value: 0.33491928","epoch:  10<br />value: 0.35267427","epoch:  11<br />value: 0.33964538","epoch:  12<br />value: 0.29713459","epoch:  13<br />value: 0.30595530","epoch:  14<br />value: 0.31406849","epoch:  15<br />value: 0.44652517","epoch:  16<br />value: 0.34211197","epoch:  17<br />value: 0.34788101","epoch:  18<br />value: 0.41088064","epoch:  19<br />value: 0.32463476","epoch:  20<br />value: 0.32487950","epoch:  21<br />value: 0.23028069","epoch:  22<br />value: 0.31719495","epoch:  23<br />value: 0.30924440","epoch:  24<br />value: 0.33769583","epoch:  25<br />value: 0.25229537","epoch:  26<br />value: 0.27538464","epoch:  27<br />value: 0.19676417","epoch:  28<br />value: 0.34010671","epoch:  29<br />value: 0.24675334","epoch:  30<br />value: 0.28560884","epoch:  31<br />value: 0.24761049","epoch:  32<br />value: 0.30301548","epoch:  33<br />value: 0.29170614","epoch:  34<br />value: 0.21052633","epoch:  35<br />value: 0.21640072","epoch:  36<br />value: 0.20409398","epoch:  37<br />value: 0.26407891","epoch:  38<br />value: 0.25867515","epoch:  39<br />value: 0.30013520","epoch:  40<br />value: 0.24708330","epoch:  41<br />value: 0.22839374","epoch:  42<br />value: 0.17126336","epoch:  43<br />value: 0.20979499","epoch:  44<br />value: 0.27283067","epoch:  45<br />value: 0.17003209","epoch:  46<br />value: 0.18333424","epoch:  47<br />value: 0.33189504","epoch:  48<br />value: 0.41015468","epoch:  49<br />value: 0.31250350","epoch:  50<br />value: 0.15318716","epoch:  51<br />value: 0.17391457","epoch:  52<br />value: 0.26968401","epoch:  53<br />value: 0.37710948","epoch:  54<br />value: 0.30998837","epoch:  55<br />value: 0.17943949","epoch:  56<br />value: 0.18321820","epoch:  57<br />value: 0.19723857","epoch:  58<br />value: 0.25082932","epoch:  59<br />value: 0.25564348","epoch:  60<br />value: 0.17463853","epoch:  61<br />value: 0.20171389","epoch:  62<br />value: 0.15079071","epoch:  63<br />value: 0.21782913","epoch:  64<br />value: 0.17345468","epoch:  65<br />value: 0.24377259","epoch:  66<br />value: 0.18442607","epoch:  67<br />value: 0.13298624","epoch:  68<br />value: 0.17349507","epoch:  69<br />value: 0.12282242","epoch:  70<br />value: 0.14998174","epoch:  71<br />value: 0.38595070","epoch:  72<br />value: 0.40973060","epoch:  73<br />value: 0.16834329","epoch:  74<br />value: 0.19116689","epoch:  75<br />value: 0.28791280","epoch:  76<br />value: 0.20947996","epoch:  77<br />value: 0.30399925","epoch:  78<br />value: 0.26247145","epoch:  79<br />value: 0.18936352","epoch:  80<br />value: 0.16062590","epoch:  81<br />value: 0.17208232","epoch:  82<br />value: 0.15021416","epoch:  83<br />value: 0.10370662","epoch:  84<br />value: 0.19359122","epoch:  85<br />value: 0.31857579","epoch:  86<br />value: 0.13760215","epoch:  87<br />value: 0.14619370","epoch:  88<br />value: 0.29603557","epoch:  89<br />value: 0.21452141","epoch:  90<br />value: 0.11649053","epoch:  91<br />value: 0.10336390","epoch:  92<br />value: 0.14795107","epoch:  93<br />value: 0.14970728","epoch:  94<br />value: 0.11051953","epoch:  95<br />value: 0.29429937","epoch:  96<br />value: 0.18861348","epoch:  97<br />value: 0.18116506","epoch:  98<br />value: 0.13263202","epoch:  99<br />value: 0.09669888","epoch: 100<br />value: 0.12231343","epoch: 101<br />value: 0.09354882","epoch: 102<br />value: 0.11399652","epoch: 103<br />value: 0.11889292","epoch: 104<br />value: 0.09837046","epoch: 105<br />value: 0.08580934","epoch: 106<br />value: 0.11080327","epoch: 107<br />value: 0.11276274","epoch: 108<br />value: 0.23658500","epoch: 109<br />value: 0.16991796","epoch: 110<br />value: 0.20804540","epoch: 111<br />value: 0.16566087","epoch: 112<br />value: 0.13122194","epoch: 113<br />value: 0.15262458","epoch: 114<br />value: 0.15825324","epoch: 115<br />value: 0.07623733","epoch: 116<br />value: 0.13040301","epoch: 117<br />value: 0.10786466","epoch: 118<br />value: 0.07791842","epoch: 119<br />value: 0.07285285","epoch: 120<br />value: 0.09526548","epoch: 121<br />value: 0.07884907","epoch: 122<br />value: 0.10080602","epoch: 123<br />value: 0.11666857","epoch: 124<br />value: 0.16102891","epoch: 125<br />value: 0.25641078","epoch: 126<br />value: 0.37649575","epoch: 127<br />value: 0.30351780","epoch: 128<br />value: 0.27804679","epoch: 129<br />value: 0.21114674","epoch: 130<br />value: 0.18203115","epoch: 131<br />value: 0.19712524","epoch: 132<br />value: 0.06947706","epoch: 133<br />value: 0.10872472","epoch: 134<br />value: 0.10267330","epoch: 135<br />value: 0.09344960","epoch: 136<br />value: 0.10459366","epoch: 137<br />value: 0.06108732","epoch: 138<br />value: 0.09633028","epoch: 139<br />value: 0.20660980","epoch: 140<br />value: 0.14237317","epoch: 141<br />value: 0.09383889","epoch: 142<br />value: 0.10624217","epoch: 143<br />value: 0.19178199","epoch: 144<br />value: 0.14823042","epoch: 145<br />value: 0.18380321","epoch: 146<br />value: 0.15730334","epoch: 147<br />value: 0.08890978","epoch: 148<br />value: 0.06079506","epoch: 149<br />value: 0.12668269","epoch: 150<br />value: 0.08948637","epoch: 151<br />value: 0.09585215","epoch: 152<br />value: 0.10081491","epoch: 153<br />value: 0.09684297","epoch: 154<br />value: 0.10966884","epoch: 155<br />value: 0.17388495","epoch: 156<br />value: 0.10659319","epoch: 157<br />value: 0.09495738","epoch: 158<br />value: 0.11356261","epoch: 159<br />value: 0.14088627","epoch: 160<br />value: 0.13953657","epoch: 161<br />value: 0.07741359","epoch: 162<br />value: 0.12896749","epoch: 163<br />value: 0.19962220","epoch: 164<br />value: 0.07001266","epoch: 165<br />value: 0.10770023","epoch: 166<br />value: 0.15265484","epoch: 167<br />value: 0.08959646","epoch: 168<br />value: 0.16124941","epoch: 169<br />value: 0.09169365","epoch: 170<br />value: 0.05058249","epoch: 171<br />value: 0.12251221","epoch: 172<br />value: 0.09404675","epoch: 173<br />value: 0.11068530","epoch: 174<br />value: 0.15645807","epoch: 175<br />value: 0.07076675","epoch: 176<br />value: 0.06475994","epoch: 177<br />value: 0.08838470","epoch: 178<br />value: 0.06286276","epoch: 179<br />value: 0.12180188","epoch: 180<br />value: 0.05016228","epoch: 181<br />value: 0.06692244","epoch: 182<br />value: 0.09018704","epoch: 183<br />value: 0.06006260","epoch: 184<br />value: 0.08103783","epoch: 185<br />value: 0.11721797","epoch: 186<br />value: 0.15054593","epoch: 187<br />value: 0.11176663","epoch: 188<br />value: 0.09955963","epoch: 189<br />value: 0.14764884","epoch: 190<br />value: 0.12589272","epoch: 191<br />value: 0.10385796","epoch: 192<br />value: 0.13843523","epoch: 193<br />value: 0.07625642","epoch: 194<br />value: 0.07477469","epoch: 195<br />value: 0.15577442","epoch: 196<br />value: 0.08425434","epoch: 197<br />value: 0.10490246","epoch: 198<br />value: 0.16197719","epoch: 199<br />value: 0.10525422","epoch: 200<br />value: 0.08371284","epoch: 201<br />value: 0.04648937","epoch: 202<br />value: 0.13895579","epoch: 203<br />value: 0.06343280","epoch: 204<br />value: 0.06309258","epoch: 205<br />value: 0.07128560","epoch: 206<br />value: 0.08472014","epoch: 207<br />value: 0.07978929","epoch: 208<br />value: 0.09269239","epoch: 209<br />value: 0.05776762","epoch: 210<br />value: 0.09124804","epoch: 211<br />value: 0.06645954","epoch: 212<br />value: 0.10711112","epoch: 213<br />value: 0.11816061","epoch: 214<br />value: 0.07377926","epoch: 215<br />value: 0.12902023","epoch: 216<br />value: 0.34266579","epoch: 217<br />value: 0.13131364","epoch: 218<br />value: 0.14015538","epoch: 219<br />value: 0.11645477","epoch: 220<br />value: 0.20197465","epoch: 221<br />value: 0.15275380","epoch: 222<br />value: 0.16916453","epoch: 223<br />value: 0.07120601","epoch: 224<br />value: 0.08743424","epoch: 225<br />value: 0.09624426","epoch: 226<br />value: 0.25522570","epoch: 227<br />value: 0.07810150","epoch: 228<br />value: 0.11683434","epoch: 229<br />value: 0.13339319","epoch: 230<br />value: 0.11496085","epoch: 231<br />value: 0.12147735","epoch: 232<br />value: 0.14868213","epoch: 233<br />value: 0.09339711","epoch: 234<br />value: 0.10641326","epoch: 235<br />value: 0.11879209","epoch: 236<br />value: 0.14572441","epoch: 237<br />value: 0.26416738","epoch: 238<br />value: 0.24610114","epoch: 239<br />value: 0.22999832","epoch: 240<br />value: 0.13376551","epoch: 241<br />value: 0.11419035","epoch: 242<br />value: 0.05610862","epoch: 243<br />value: 0.07476837","epoch: 244<br />value: 0.13215476","epoch: 245<br />value: 0.06929587","epoch: 246<br />value: 0.07130749","epoch: 247<br />value: 0.12793044","epoch: 248<br />value: 0.11474959","epoch: 249<br />value: 0.21181149","epoch: 250<br />value: 0.11440018","epoch: 251<br />value: 0.15501805","epoch: 252<br />value: 0.10963330","epoch: 253<br />value: 0.07140461","epoch: 254<br />value: 0.07586811","epoch: 255<br />value: 0.12728161","epoch: 256<br />value: 0.16040148","epoch: 257<br />value: 0.05920617","epoch: 258<br />value: 0.07271061","epoch: 259<br />value: 0.04143484","epoch: 260<br />value: 0.09548302","epoch: 261<br />value: 0.06538549","epoch: 262<br />value: 0.05121073","epoch: 263<br />value: 0.08903897","epoch: 264<br />value: 0.09891571","epoch: 265<br />value: 0.07694811","epoch: 266<br />value: 0.04480475","epoch: 267<br />value: 0.09096826","epoch: 268<br />value: 0.06971219","epoch: 269<br />value: 0.10086954","epoch: 270<br />value: 0.04902641","epoch: 271<br />value: 0.06921810","epoch: 272<br />value: 0.05608222","epoch: 273<br />value: 0.07197064","epoch: 274<br />value: 0.06650475","epoch: 275<br />value: 0.06669159","epoch: 276<br />value: 0.13698808","epoch: 277<br />value: 0.07297753","epoch: 278<br />value: 0.04271206","epoch: 279<br />value: 0.08181811","epoch: 280<br />value: 0.09067756","epoch: 281<br />value: 0.05794403","epoch: 282<br />value: 0.03696380","epoch: 283<br />value: 0.06146798","epoch: 284<br />value: 0.03071374","epoch: 285<br />value: 0.06481125","epoch: 286<br />value: 0.02449856","epoch: 287<br />value: 0.05210522","epoch: 288<br />value: 0.04799140","epoch: 289<br />value: 0.05343416","epoch: 290<br />value: 0.06581956","epoch: 291<br />value: 0.07050861","epoch: 292<br />value: 0.10610586","epoch: 293<br />value: 0.04462566","epoch: 294<br />value: 0.08107148","epoch: 295<br />value: 0.10342976","epoch: 296<br />value: 0.05326001","epoch: 297<br />value: 0.05951581","epoch: 298<br />value: 0.05518825","epoch: 299<br />value: 0.11819254","epoch: 300<br />value: 0.05042644"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"transparent","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300],"y":[0.863945603370667,0.857142865657806,0.911564648151398,0.857142865657806,0.843537390232086,0.816326558589935,0.863945603370667,0.884353756904602,0.850340127944946,0.843537390232086,0.877551019191742,0.911564648151398,0.904761910438538,0.911564648151398,0.823129236698151,0.857142865657806,0.877551019191742,0.836734712123871,0.857142865657806,0.870748281478882,0.897959172725677,0.870748281478882,0.904761910438538,0.863945603370667,0.938775539398193,0.877551019191742,0.938775539398193,0.857142865657806,0.931972801685333,0.931972801685333,0.891156435012817,0.897959172725677,0.863945603370667,0.925170063972473,0.925170063972473,0.938775539398193,0.904761910438538,0.904761910438538,0.904761910438538,0.897959172725677,0.925170063972473,0.945578217506409,0.925170063972473,0.918367326259613,0.938775539398193,0.931972801685333,0.911564648151398,0.857142865657806,0.897959172725677,0.965986371040344,0.945578217506409,0.897959172725677,0.870748281478882,0.897959172725677,0.952380955219269,0.925170063972473,0.952380955219269,0.911564648151398,0.897959172725677,0.938775539398193,0.904761910438538,0.938775539398193,0.925170063972473,0.938775539398193,0.918367326259613,0.952380955219269,0.945578217506409,0.904761910438538,0.959183692932129,0.938775539398193,0.857142865657806,0.857142865657806,0.965986371040344,0.918367326259613,0.931972801685333,0.911564648151398,0.884353756904602,0.918367326259613,0.925170063972473,0.952380955219269,0.911564648151398,0.945578217506409,0.965986371040344,0.925170063972473,0.891156435012817,0.979591846466064,0.938775539398193,0.911564648151398,0.945578217506409,0.965986371040344,0.959183692932129,0.945578217506409,0.945578217506409,0.952380955219269,0.904761910438538,0.931972801685333,0.945578217506409,0.952380955219269,0.952380955219269,0.959183692932129,0.972789108753204,0.972789108753204,0.952380955219269,0.972789108753204,0.959183692932129,0.959183692932129,0.972789108753204,0.904761910438538,0.938775539398193,0.911564648151398,0.959183692932129,0.959183692932129,0.938775539398193,0.945578217506409,0.979591846466064,0.965986371040344,0.972789108753204,0.959183692932129,0.972789108753204,0.959183692932129,0.99319726228714,0.979591846466064,0.952380955219269,0.959183692932129,0.945578217506409,0.897959172725677,0.897959172725677,0.904761910438538,0.925170063972473,0.925170063972473,0.938775539398193,0.986394584178925,0.965986371040344,0.938775539398193,0.979591846466064,0.952380955219269,0.99319726228714,0.952380955219269,0.911564648151398,0.938775539398193,0.965986371040344,0.945578217506409,0.945578217506409,0.959183692932129,0.931972801685333,0.945578217506409,0.952380955219269,0.979591846466064,0.959183692932129,0.965986371040344,0.965986371040344,0.965986371040344,0.959183692932129,0.952380955219269,0.945578217506409,0.952380955219269,0.979591846466064,0.952380955219269,0.945578217506409,0.965986371040344,0.972789108753204,0.945578217506409,0.918367326259613,0.979591846466064,0.938775539398193,0.945578217506409,0.965986371040344,0.931972801685333,0.979591846466064,0.979591846466064,0.965986371040344,0.959183692932129,0.952380955219269,0.979591846466064,0.979591846466064,0.972789108753204,0.986394584178925,0.986394584178925,0.972789108753204,0.979591846466064,0.979591846466064,0.965986371040344,0.986394584178925,0.959183692932129,0.972789108753204,0.945578217506409,0.965986371040344,0.938775539398193,0.952380955219269,0.959183692932129,0.979591846466064,0.959183692932129,0.972789108753204,0.965986371040344,0.911564648151398,0.965986371040344,0.972789108753204,0.945578217506409,0.965986371040344,0.979591846466064,0.99319726228714,0.945578217506409,0.972789108753204,0.979591846466064,0.972789108753204,0.986394584178925,0.972789108753204,0.965986371040344,0.979591846466064,0.965986371040344,0.972789108753204,0.959183692932129,0.965986371040344,0.979591846466064,0.945578217506409,0.904761910438538,0.959183692932129,0.952380955219269,0.959183692932129,0.918367326259613,0.931972801685333,0.945578217506409,0.986394584178925,0.965986371040344,0.965986371040344,0.938775539398193,0.979591846466064,0.945578217506409,0.952380955219269,0.952380955219269,0.965986371040344,0.945578217506409,0.965986371040344,0.972789108753204,0.952380955219269,0.938775539398193,0.918367326259613,0.925170063972473,0.884353756904602,0.959183692932129,0.972789108753204,0.986394584178925,0.986394584178925,0.959183692932129,0.979591846466064,0.979591846466064,0.965986371040344,0.965986371040344,0.931972801685333,0.952380955219269,0.952380955219269,0.959183692932129,0.979591846466064,0.972789108753204,0.945578217506409,0.959183692932129,0.979591846466064,0.972789108753204,0.979591846466064,0.965986371040344,0.979591846466064,0.979591846466064,0.972789108753204,0.972789108753204,0.972789108753204,0.979591846466064,0.979591846466064,0.965986371040344,0.965986371040344,0.979591846466064,0.972789108753204,0.979591846466064,0.972789108753204,0.979591846466064,0.972789108753204,0.952380955219269,0.972789108753204,0.99319726228714,0.972789108753204,0.972789108753204,0.972789108753204,0.99319726228714,0.972789108753204,0.99319726228714,0.972789108753204,1,0.979591846466064,0.986394584178925,0.979591846466064,0.972789108753204,0.979591846466064,0.965986371040344,0.979591846466064,0.972789108753204,0.972789108753204,0.986394584178925,0.979591846466064,0.986394584178925,0.959183692932129,0.979591846466064],"text":["epoch:   1<br />value: 0.86394560","epoch:   2<br />value: 0.85714287","epoch:   3<br />value: 0.91156465","epoch:   4<br />value: 0.85714287","epoch:   5<br />value: 0.84353739","epoch:   6<br />value: 0.81632656","epoch:   7<br />value: 0.86394560","epoch:   8<br />value: 0.88435376","epoch:   9<br />value: 0.85034013","epoch:  10<br />value: 0.84353739","epoch:  11<br />value: 0.87755102","epoch:  12<br />value: 0.91156465","epoch:  13<br />value: 0.90476191","epoch:  14<br />value: 0.91156465","epoch:  15<br />value: 0.82312924","epoch:  16<br />value: 0.85714287","epoch:  17<br />value: 0.87755102","epoch:  18<br />value: 0.83673471","epoch:  19<br />value: 0.85714287","epoch:  20<br />value: 0.87074828","epoch:  21<br />value: 0.89795917","epoch:  22<br />value: 0.87074828","epoch:  23<br />value: 0.90476191","epoch:  24<br />value: 0.86394560","epoch:  25<br />value: 0.93877554","epoch:  26<br />value: 0.87755102","epoch:  27<br />value: 0.93877554","epoch:  28<br />value: 0.85714287","epoch:  29<br />value: 0.93197280","epoch:  30<br />value: 0.93197280","epoch:  31<br />value: 0.89115644","epoch:  32<br />value: 0.89795917","epoch:  33<br />value: 0.86394560","epoch:  34<br />value: 0.92517006","epoch:  35<br />value: 0.92517006","epoch:  36<br />value: 0.93877554","epoch:  37<br />value: 0.90476191","epoch:  38<br />value: 0.90476191","epoch:  39<br />value: 0.90476191","epoch:  40<br />value: 0.89795917","epoch:  41<br />value: 0.92517006","epoch:  42<br />value: 0.94557822","epoch:  43<br />value: 0.92517006","epoch:  44<br />value: 0.91836733","epoch:  45<br />value: 0.93877554","epoch:  46<br />value: 0.93197280","epoch:  47<br />value: 0.91156465","epoch:  48<br />value: 0.85714287","epoch:  49<br />value: 0.89795917","epoch:  50<br />value: 0.96598637","epoch:  51<br />value: 0.94557822","epoch:  52<br />value: 0.89795917","epoch:  53<br />value: 0.87074828","epoch:  54<br />value: 0.89795917","epoch:  55<br />value: 0.95238096","epoch:  56<br />value: 0.92517006","epoch:  57<br />value: 0.95238096","epoch:  58<br />value: 0.91156465","epoch:  59<br />value: 0.89795917","epoch:  60<br />value: 0.93877554","epoch:  61<br />value: 0.90476191","epoch:  62<br />value: 0.93877554","epoch:  63<br />value: 0.92517006","epoch:  64<br />value: 0.93877554","epoch:  65<br />value: 0.91836733","epoch:  66<br />value: 0.95238096","epoch:  67<br />value: 0.94557822","epoch:  68<br />value: 0.90476191","epoch:  69<br />value: 0.95918369","epoch:  70<br />value: 0.93877554","epoch:  71<br />value: 0.85714287","epoch:  72<br />value: 0.85714287","epoch:  73<br />value: 0.96598637","epoch:  74<br />value: 0.91836733","epoch:  75<br />value: 0.93197280","epoch:  76<br />value: 0.91156465","epoch:  77<br />value: 0.88435376","epoch:  78<br />value: 0.91836733","epoch:  79<br />value: 0.92517006","epoch:  80<br />value: 0.95238096","epoch:  81<br />value: 0.91156465","epoch:  82<br />value: 0.94557822","epoch:  83<br />value: 0.96598637","epoch:  84<br />value: 0.92517006","epoch:  85<br />value: 0.89115644","epoch:  86<br />value: 0.97959185","epoch:  87<br />value: 0.93877554","epoch:  88<br />value: 0.91156465","epoch:  89<br />value: 0.94557822","epoch:  90<br />value: 0.96598637","epoch:  91<br />value: 0.95918369","epoch:  92<br />value: 0.94557822","epoch:  93<br />value: 0.94557822","epoch:  94<br />value: 0.95238096","epoch:  95<br />value: 0.90476191","epoch:  96<br />value: 0.93197280","epoch:  97<br />value: 0.94557822","epoch:  98<br />value: 0.95238096","epoch:  99<br />value: 0.95238096","epoch: 100<br />value: 0.95918369","epoch: 101<br />value: 0.97278911","epoch: 102<br />value: 0.97278911","epoch: 103<br />value: 0.95238096","epoch: 104<br />value: 0.97278911","epoch: 105<br />value: 0.95918369","epoch: 106<br />value: 0.95918369","epoch: 107<br />value: 0.97278911","epoch: 108<br />value: 0.90476191","epoch: 109<br />value: 0.93877554","epoch: 110<br />value: 0.91156465","epoch: 111<br />value: 0.95918369","epoch: 112<br />value: 0.95918369","epoch: 113<br />value: 0.93877554","epoch: 114<br />value: 0.94557822","epoch: 115<br />value: 0.97959185","epoch: 116<br />value: 0.96598637","epoch: 117<br />value: 0.97278911","epoch: 118<br />value: 0.95918369","epoch: 119<br />value: 0.97278911","epoch: 120<br />value: 0.95918369","epoch: 121<br />value: 0.99319726","epoch: 122<br />value: 0.97959185","epoch: 123<br />value: 0.95238096","epoch: 124<br />value: 0.95918369","epoch: 125<br />value: 0.94557822","epoch: 126<br />value: 0.89795917","epoch: 127<br />value: 0.89795917","epoch: 128<br />value: 0.90476191","epoch: 129<br />value: 0.92517006","epoch: 130<br />value: 0.92517006","epoch: 131<br />value: 0.93877554","epoch: 132<br />value: 0.98639458","epoch: 133<br />value: 0.96598637","epoch: 134<br />value: 0.93877554","epoch: 135<br />value: 0.97959185","epoch: 136<br />value: 0.95238096","epoch: 137<br />value: 0.99319726","epoch: 138<br />value: 0.95238096","epoch: 139<br />value: 0.91156465","epoch: 140<br />value: 0.93877554","epoch: 141<br />value: 0.96598637","epoch: 142<br />value: 0.94557822","epoch: 143<br />value: 0.94557822","epoch: 144<br />value: 0.95918369","epoch: 145<br />value: 0.93197280","epoch: 146<br />value: 0.94557822","epoch: 147<br />value: 0.95238096","epoch: 148<br />value: 0.97959185","epoch: 149<br />value: 0.95918369","epoch: 150<br />value: 0.96598637","epoch: 151<br />value: 0.96598637","epoch: 152<br />value: 0.96598637","epoch: 153<br />value: 0.95918369","epoch: 154<br />value: 0.95238096","epoch: 155<br />value: 0.94557822","epoch: 156<br />value: 0.95238096","epoch: 157<br />value: 0.97959185","epoch: 158<br />value: 0.95238096","epoch: 159<br />value: 0.94557822","epoch: 160<br />value: 0.96598637","epoch: 161<br />value: 0.97278911","epoch: 162<br />value: 0.94557822","epoch: 163<br />value: 0.91836733","epoch: 164<br />value: 0.97959185","epoch: 165<br />value: 0.93877554","epoch: 166<br />value: 0.94557822","epoch: 167<br />value: 0.96598637","epoch: 168<br />value: 0.93197280","epoch: 169<br />value: 0.97959185","epoch: 170<br />value: 0.97959185","epoch: 171<br />value: 0.96598637","epoch: 172<br />value: 0.95918369","epoch: 173<br />value: 0.95238096","epoch: 174<br />value: 0.97959185","epoch: 175<br />value: 0.97959185","epoch: 176<br />value: 0.97278911","epoch: 177<br />value: 0.98639458","epoch: 178<br />value: 0.98639458","epoch: 179<br />value: 0.97278911","epoch: 180<br />value: 0.97959185","epoch: 181<br />value: 0.97959185","epoch: 182<br />value: 0.96598637","epoch: 183<br />value: 0.98639458","epoch: 184<br />value: 0.95918369","epoch: 185<br />value: 0.97278911","epoch: 186<br />value: 0.94557822","epoch: 187<br />value: 0.96598637","epoch: 188<br />value: 0.93877554","epoch: 189<br />value: 0.95238096","epoch: 190<br />value: 0.95918369","epoch: 191<br />value: 0.97959185","epoch: 192<br />value: 0.95918369","epoch: 193<br />value: 0.97278911","epoch: 194<br />value: 0.96598637","epoch: 195<br />value: 0.91156465","epoch: 196<br />value: 0.96598637","epoch: 197<br />value: 0.97278911","epoch: 198<br />value: 0.94557822","epoch: 199<br />value: 0.96598637","epoch: 200<br />value: 0.97959185","epoch: 201<br />value: 0.99319726","epoch: 202<br />value: 0.94557822","epoch: 203<br />value: 0.97278911","epoch: 204<br />value: 0.97959185","epoch: 205<br />value: 0.97278911","epoch: 206<br />value: 0.98639458","epoch: 207<br />value: 0.97278911","epoch: 208<br />value: 0.96598637","epoch: 209<br />value: 0.97959185","epoch: 210<br />value: 0.96598637","epoch: 211<br />value: 0.97278911","epoch: 212<br />value: 0.95918369","epoch: 213<br />value: 0.96598637","epoch: 214<br />value: 0.97959185","epoch: 215<br />value: 0.94557822","epoch: 216<br />value: 0.90476191","epoch: 217<br />value: 0.95918369","epoch: 218<br />value: 0.95238096","epoch: 219<br />value: 0.95918369","epoch: 220<br />value: 0.91836733","epoch: 221<br />value: 0.93197280","epoch: 222<br />value: 0.94557822","epoch: 223<br />value: 0.98639458","epoch: 224<br />value: 0.96598637","epoch: 225<br />value: 0.96598637","epoch: 226<br />value: 0.93877554","epoch: 227<br />value: 0.97959185","epoch: 228<br />value: 0.94557822","epoch: 229<br />value: 0.95238096","epoch: 230<br />value: 0.95238096","epoch: 231<br />value: 0.96598637","epoch: 232<br />value: 0.94557822","epoch: 233<br />value: 0.96598637","epoch: 234<br />value: 0.97278911","epoch: 235<br />value: 0.95238096","epoch: 236<br />value: 0.93877554","epoch: 237<br />value: 0.91836733","epoch: 238<br />value: 0.92517006","epoch: 239<br />value: 0.88435376","epoch: 240<br />value: 0.95918369","epoch: 241<br />value: 0.97278911","epoch: 242<br />value: 0.98639458","epoch: 243<br />value: 0.98639458","epoch: 244<br />value: 0.95918369","epoch: 245<br />value: 0.97959185","epoch: 246<br />value: 0.97959185","epoch: 247<br />value: 0.96598637","epoch: 248<br />value: 0.96598637","epoch: 249<br />value: 0.93197280","epoch: 250<br />value: 0.95238096","epoch: 251<br />value: 0.95238096","epoch: 252<br />value: 0.95918369","epoch: 253<br />value: 0.97959185","epoch: 254<br />value: 0.97278911","epoch: 255<br />value: 0.94557822","epoch: 256<br />value: 0.95918369","epoch: 257<br />value: 0.97959185","epoch: 258<br />value: 0.97278911","epoch: 259<br />value: 0.97959185","epoch: 260<br />value: 0.96598637","epoch: 261<br />value: 0.97959185","epoch: 262<br />value: 0.97959185","epoch: 263<br />value: 0.97278911","epoch: 264<br />value: 0.97278911","epoch: 265<br />value: 0.97278911","epoch: 266<br />value: 0.97959185","epoch: 267<br />value: 0.97959185","epoch: 268<br />value: 0.96598637","epoch: 269<br />value: 0.96598637","epoch: 270<br />value: 0.97959185","epoch: 271<br />value: 0.97278911","epoch: 272<br />value: 0.97959185","epoch: 273<br />value: 0.97278911","epoch: 274<br />value: 0.97959185","epoch: 275<br />value: 0.97278911","epoch: 276<br />value: 0.95238096","epoch: 277<br />value: 0.97278911","epoch: 278<br />value: 0.99319726","epoch: 279<br />value: 0.97278911","epoch: 280<br />value: 0.97278911","epoch: 281<br />value: 0.97278911","epoch: 282<br />value: 0.99319726","epoch: 283<br />value: 0.97278911","epoch: 284<br />value: 0.99319726","epoch: 285<br />value: 0.97278911","epoch: 286<br />value: 1.00000000","epoch: 287<br />value: 0.97959185","epoch: 288<br />value: 0.98639458","epoch: 289<br />value: 0.97959185","epoch: 290<br />value: 0.97278911","epoch: 291<br />value: 0.97959185","epoch: 292<br />value: 0.96598637","epoch: 293<br />value: 0.97959185","epoch: 294<br />value: 0.97278911","epoch: 295<br />value: 0.97278911","epoch: 296<br />value: 0.98639458","epoch: 297<br />value: 0.97959185","epoch: 298<br />value: 0.98639458","epoch: 299<br />value: 0.95918369","epoch: 300<br />value: 0.97959185"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"transparent","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y2","hoverinfo":"text","frame":null},{"x":[1,4.78481012658228,8.56962025316456,12.3544303797468,16.1392405063291,19.9240506329114,23.7088607594937,27.493670886076,31.2784810126582,35.0632911392405,38.8481012658228,42.6329113924051,46.4177215189873,50.2025316455696,53.9873417721519,57.7721518987342,61.5569620253165,65.3417721518987,69.126582278481,72.9113924050633,76.6962025316456,80.4810126582279,84.2658227848101,88.0506329113924,91.8354430379747,95.620253164557,99.4050632911392,103.189873417722,106.974683544304,110.759493670886,114.544303797468,118.329113924051,122.113924050633,125.898734177215,129.683544303797,133.46835443038,137.253164556962,141.037974683544,144.822784810127,148.607594936709,152.392405063291,156.177215189873,159.962025316456,163.746835443038,167.53164556962,171.316455696203,175.101265822785,178.886075949367,182.670886075949,186.455696202532,190.240506329114,194.025316455696,197.810126582278,201.594936708861,205.379746835443,209.164556962025,212.949367088608,216.73417721519,220.518987341772,224.303797468354,228.088607594937,231.873417721519,235.658227848101,239.443037974684,243.227848101266,247.012658227848,250.79746835443,254.582278481013,258.367088607595,262.151898734177,265.936708860759,269.721518987342,273.506329113924,277.291139240506,281.075949367089,284.860759493671,288.645569620253,292.430379746835,296.215189873418,300],"y":[0.373530112959425,0.361878979279773,0.35049734309403,0.339389686745384,0.328560492577024,0.318014242932137,0.307755420153912,0.297788506585536,0.288117984570198,0.278748336451086,0.269684423391286,0.260936990097315,0.252500991239639,0.244365270110912,0.23651867000379,0.228950034210926,0.221648206024976,0.214602028738594,0.207800345644436,0.201232000035155,0.194919799892854,0.189086649636102,0.183722132414683,0.178761619983542,0.174140484097624,0.169794096511872,0.165657828981232,0.161667053260646,0.15775714110506,0.153863464269418,0.149927161186231,0.14597151904186,0.142048188554237,0.138203141953724,0.134482351470684,0.130931789335478,0.127597427778469,0.124525239030019,0.121761195320489,0.119351268880243,0.117380209348935,0.115970595585027,0.115037148549324,0.114478121713951,0.114191768551033,0.114076342532694,0.114030097131059,0.113951285818252,0.113738162066397,0.113288979347619,0.112626707015673,0.112185429641024,0.111941099882095,0.111810946011649,0.111712196302452,0.111562079027266,0.111277822458855,0.110776654869984,0.109975804533417,0.108792499721916,0.107311280235696,0.105810876147072,0.104280032425245,0.102699350393637,0.10104943137567,0.0993108766947656,0.0974642876743473,0.0954902656378371,0.0933694119086572,0.0910823278102301,0.0886174952202795,0.085987762967359,0.0832019442879771,0.0802684277106354,0.0771956017638359,0.0739918549760803,0.0706655758758703,0.0672251529917077,0.0636789748520943,0.0600354299855319],"text":["epoch:   1.00000<br />value: 0.37353011","epoch:   4.78481<br />value: 0.36187898","epoch:   8.56962<br />value: 0.35049734","epoch:  12.35443<br />value: 0.33938969","epoch:  16.13924<br />value: 0.32856049","epoch:  19.92405<br />value: 0.31801424","epoch:  23.70886<br />value: 0.30775542","epoch:  27.49367<br />value: 0.29778851","epoch:  31.27848<br />value: 0.28811798","epoch:  35.06329<br />value: 0.27874834","epoch:  38.84810<br />value: 0.26968442","epoch:  42.63291<br />value: 0.26093699","epoch:  46.41772<br />value: 0.25250099","epoch:  50.20253<br />value: 0.24436527","epoch:  53.98734<br />value: 0.23651867","epoch:  57.77215<br />value: 0.22895003","epoch:  61.55696<br />value: 0.22164821","epoch:  65.34177<br />value: 0.21460203","epoch:  69.12658<br />value: 0.20780035","epoch:  72.91139<br />value: 0.20123200","epoch:  76.69620<br />value: 0.19491980","epoch:  80.48101<br />value: 0.18908665","epoch:  84.26582<br />value: 0.18372213","epoch:  88.05063<br />value: 0.17876162","epoch:  91.83544<br />value: 0.17414048","epoch:  95.62025<br />value: 0.16979410","epoch:  99.40506<br />value: 0.16565783","epoch: 103.18987<br />value: 0.16166705","epoch: 106.97468<br />value: 0.15775714","epoch: 110.75949<br />value: 0.15386346","epoch: 114.54430<br />value: 0.14992716","epoch: 118.32911<br />value: 0.14597152","epoch: 122.11392<br />value: 0.14204819","epoch: 125.89873<br />value: 0.13820314","epoch: 129.68354<br />value: 0.13448235","epoch: 133.46835<br />value: 0.13093179","epoch: 137.25316<br />value: 0.12759743","epoch: 141.03797<br />value: 0.12452524","epoch: 144.82278<br />value: 0.12176120","epoch: 148.60759<br />value: 0.11935127","epoch: 152.39241<br />value: 0.11738021","epoch: 156.17722<br />value: 0.11597060","epoch: 159.96203<br />value: 0.11503715","epoch: 163.74684<br />value: 0.11447812","epoch: 167.53165<br />value: 0.11419177","epoch: 171.31646<br />value: 0.11407634","epoch: 175.10127<br />value: 0.11403010","epoch: 178.88608<br />value: 0.11395129","epoch: 182.67089<br />value: 0.11373816","epoch: 186.45570<br />value: 0.11328898","epoch: 190.24051<br />value: 0.11262671","epoch: 194.02532<br />value: 0.11218543","epoch: 197.81013<br />value: 0.11194110","epoch: 201.59494<br />value: 0.11181095","epoch: 205.37975<br />value: 0.11171220","epoch: 209.16456<br />value: 0.11156208","epoch: 212.94937<br />value: 0.11127782","epoch: 216.73418<br />value: 0.11077665","epoch: 220.51899<br />value: 0.10997580","epoch: 224.30380<br />value: 0.10879250","epoch: 228.08861<br />value: 0.10731128","epoch: 231.87342<br />value: 0.10581088","epoch: 235.65823<br />value: 0.10428003","epoch: 239.44304<br />value: 0.10269935","epoch: 243.22785<br />value: 0.10104943","epoch: 247.01266<br />value: 0.09931088","epoch: 250.79747<br />value: 0.09746429","epoch: 254.58228<br />value: 0.09549027","epoch: 258.36709<br />value: 0.09336941","epoch: 262.15190<br />value: 0.09108233","epoch: 265.93671<br />value: 0.08861750","epoch: 269.72152<br />value: 0.08598776","epoch: 273.50633<br />value: 0.08320194","epoch: 277.29114<br />value: 0.08026843","epoch: 281.07595<br />value: 0.07719560","epoch: 284.86076<br />value: 0.07399185","epoch: 288.64557<br />value: 0.07066558","epoch: 292.43038<br />value: 0.06722515","epoch: 296.21519<br />value: 0.06367897","epoch: 300.00000<br />value: 0.06003543"],"type":"scatter","mode":"lines","name":"fitted values","line":{"width":3.77952755905512,"color":"rgba(51,102,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,4.78481012658228,8.56962025316456,12.3544303797468,16.1392405063291,19.9240506329114,23.7088607594937,27.493670886076,31.2784810126582,35.0632911392405,38.8481012658228,42.6329113924051,46.4177215189873,50.2025316455696,53.9873417721519,57.7721518987342,61.5569620253165,65.3417721518987,69.126582278481,72.9113924050633,76.6962025316456,80.4810126582279,84.2658227848101,88.0506329113924,91.8354430379747,95.620253164557,99.4050632911392,103.189873417722,106.974683544304,110.759493670886,114.544303797468,118.329113924051,122.113924050633,125.898734177215,129.683544303797,133.46835443038,137.253164556962,141.037974683544,144.822784810127,148.607594936709,152.392405063291,156.177215189873,159.962025316456,163.746835443038,167.53164556962,171.316455696203,175.101265822785,178.886075949367,182.670886075949,186.455696202532,190.240506329114,194.025316455696,197.810126582278,201.594936708861,205.379746835443,209.164556962025,212.949367088608,216.73417721519,220.518987341772,224.303797468354,228.088607594937,231.873417721519,235.658227848101,239.443037974684,243.227848101266,247.012658227848,250.79746835443,254.582278481013,258.367088607595,262.151898734177,265.936708860759,269.721518987342,273.506329113924,277.291139240506,281.075949367089,284.860759493671,288.645569620253,292.430379746835,296.215189873418,300],"y":[0.862835812376777,0.867473968706814,0.871998231595892,0.876406792022149,0.880697840963725,0.884869569398757,0.888920168305384,0.892847828661746,0.896650741445979,0.900327097636225,0.903874939888294,0.907290070376248,0.910574638016515,0.91373320993437,0.916770353255092,0.919690635103959,0.922498622606246,0.925198882887232,0.927795983072195,0.930294490286411,0.932683487542921,0.934860998935014,0.93683169629033,0.938624989422107,0.940270288143587,0.941797002268008,0.943234541608611,0.944612315978637,0.945959735191324,0.947306209059914,0.948675258754198,0.950030847032476,0.951352971440545,0.952631728277604,0.95385721384285,0.955019524435484,0.956108756354702,0.957115005899703,0.958028369369685,0.958838943063846,0.95951499473838,0.959988078521964,0.960288227910896,0.960453962839857,0.960523803243534,0.96053626905661,0.960529880213769,0.960543156649695,0.960614618299073,0.960782785096587,0.961042691368088,0.961242941721651,0.961387867527529,0.961502073487704,0.961610164304156,0.961736744678867,0.961906419313816,0.962143792910984,0.962473470172353,0.962920055799902,0.963458301136592,0.964005136699072,0.964563932662066,0.965140486166157,0.965740594351929,0.966370054359964,0.967034663330846,0.967740218405158,0.968492516723484,0.969297355426407,0.970158377189765,0.971072165446013,0.972036054482214,0.973047488366903,0.974103911168613,0.975202766955878,0.976341499797231,0.977517553761205,0.978728372916335,0.979971401331153],"text":["epoch:   1.00000<br />value: 0.86283581","epoch:   4.78481<br />value: 0.86747397","epoch:   8.56962<br />value: 0.87199823","epoch:  12.35443<br />value: 0.87640679","epoch:  16.13924<br />value: 0.88069784","epoch:  19.92405<br />value: 0.88486957","epoch:  23.70886<br />value: 0.88892017","epoch:  27.49367<br />value: 0.89284783","epoch:  31.27848<br />value: 0.89665074","epoch:  35.06329<br />value: 0.90032710","epoch:  38.84810<br />value: 0.90387494","epoch:  42.63291<br />value: 0.90729007","epoch:  46.41772<br />value: 0.91057464","epoch:  50.20253<br />value: 0.91373321","epoch:  53.98734<br />value: 0.91677035","epoch:  57.77215<br />value: 0.91969064","epoch:  61.55696<br />value: 0.92249862","epoch:  65.34177<br />value: 0.92519888","epoch:  69.12658<br />value: 0.92779598","epoch:  72.91139<br />value: 0.93029449","epoch:  76.69620<br />value: 0.93268349","epoch:  80.48101<br />value: 0.93486100","epoch:  84.26582<br />value: 0.93683170","epoch:  88.05063<br />value: 0.93862499","epoch:  91.83544<br />value: 0.94027029","epoch:  95.62025<br />value: 0.94179700","epoch:  99.40506<br />value: 0.94323454","epoch: 103.18987<br />value: 0.94461232","epoch: 106.97468<br />value: 0.94595974","epoch: 110.75949<br />value: 0.94730621","epoch: 114.54430<br />value: 0.94867526","epoch: 118.32911<br />value: 0.95003085","epoch: 122.11392<br />value: 0.95135297","epoch: 125.89873<br />value: 0.95263173","epoch: 129.68354<br />value: 0.95385721","epoch: 133.46835<br />value: 0.95501952","epoch: 137.25316<br />value: 0.95610876","epoch: 141.03797<br />value: 0.95711501","epoch: 144.82278<br />value: 0.95802837","epoch: 148.60759<br />value: 0.95883894","epoch: 152.39241<br />value: 0.95951499","epoch: 156.17722<br />value: 0.95998808","epoch: 159.96203<br />value: 0.96028823","epoch: 163.74684<br />value: 0.96045396","epoch: 167.53165<br />value: 0.96052380","epoch: 171.31646<br />value: 0.96053627","epoch: 175.10127<br />value: 0.96052988","epoch: 178.88608<br />value: 0.96054316","epoch: 182.67089<br />value: 0.96061462","epoch: 186.45570<br />value: 0.96078279","epoch: 190.24051<br />value: 0.96104269","epoch: 194.02532<br />value: 0.96124294","epoch: 197.81013<br />value: 0.96138787","epoch: 201.59494<br />value: 0.96150207","epoch: 205.37975<br />value: 0.96161016","epoch: 209.16456<br />value: 0.96173674","epoch: 212.94937<br />value: 0.96190642","epoch: 216.73418<br />value: 0.96214379","epoch: 220.51899<br />value: 0.96247347","epoch: 224.30380<br />value: 0.96292006","epoch: 228.08861<br />value: 0.96345830","epoch: 231.87342<br />value: 0.96400514","epoch: 235.65823<br />value: 0.96456393","epoch: 239.44304<br />value: 0.96514049","epoch: 243.22785<br />value: 0.96574059","epoch: 247.01266<br />value: 0.96637005","epoch: 250.79747<br />value: 0.96703466","epoch: 254.58228<br />value: 0.96774022","epoch: 258.36709<br />value: 0.96849252","epoch: 262.15190<br />value: 0.96929736","epoch: 265.93671<br />value: 0.97015838","epoch: 269.72152<br />value: 0.97107217","epoch: 273.50633<br />value: 0.97203605","epoch: 277.29114<br />value: 0.97304749","epoch: 281.07595<br />value: 0.97410391","epoch: 284.86076<br />value: 0.97520277","epoch: 288.64557<br />value: 0.97634150","epoch: 292.43038<br />value: 0.97751755","epoch: 296.21519<br />value: 0.97872837","epoch: 300.00000<br />value: 0.97997140"],"type":"scatter","mode":"lines","name":"fitted values","line":{"width":3.77952755905512,"color":"rgba(51,102,255,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y2","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":37.9178082191781,"r":18.9954337899543,"b":40.1826484018265,"l":43.1050228310502},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-13.95,314.95],"tickmode":"array","ticktext":["0","50","100","150","200","250","300"],"tickvals":[0,50,100,150,200,250,300],"categoryorder":"array","categoryarray":["0","50","100","150","200","250","300"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y2","title":"","hoverformat":".2f"},"annotations":[{"text":"epoch","x":0.5,"y":-0.0176940639269406,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"center","yanchor":"top","annotationType":"axis"},{"text":"value","x":-0.0159001956947162,"y":0.5,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-90,"xanchor":"right","yanchor":"center","annotationType":"axis"},{"text":"loss","x":1,"y":0.752853881278539,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(26,26,26,1)","family":"","size":11.689497716895},"xref":"paper","yref":"paper","textangle":90,"xanchor":"left","yanchor":"middle"},{"text":"acc","x":1,"y":0.247146118721461,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(26,26,26,1)","family":"","size":11.689497716895},"xref":"paper","yref":"paper","textangle":90,"xanchor":"left","yanchor":"middle"}],"yaxis":{"domain":[0.505707762557078,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.00157797813443394,0.572105946971034],"tickmode":"array","ticktext":["0.0","0.1","0.2","0.3","0.4","0.5"],"tickvals":[0,0.1,0.2,0.3,0.4,0.5],"categoryorder":"array","categoryarray":["0.0","0.1","0.2","0.3","0.4","0.5"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"","hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0.505707762557078,"y1":1},{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","y0":0.505707762557078,"y1":1,"x0":0,"x1":23.37899543379,"xanchor":1,"xsizemode":"pixel"},{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":0.494292237442922},{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","y0":0,"y1":0.494292237442922,"x0":0,"x1":23.37899543379,"xanchor":1,"xsizemode":"pixel"}],"yaxis2":{"type":"linear","autorange":false,"range":[0.807142886519432,1.0091836720705],"tickmode":"array","ticktext":["0.85","0.90","0.95","1.00"],"tickvals":[0.85,0.9,0.95,1],"categoryorder":"array","categoryarray":["0.85","0.90","0.95","1.00"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"domain":[0,0.494292237442922],"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"","hoverformat":".2f"},"showlegend":false,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"1b5b35f7090b":{"x":{},"y":{},"type":"scatter"},"1b5bf3a5f9d":{"x":{},"y":{}}},"cur_data":"1b5b35f7090b","visdat":{"1b5b35f7090b":["function (y) ","x"],"1b5bf3a5f9d":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
<strong>Fig. 1</strong>: Accuracy and loss values for an exemplary training process.
</p>
</div>
</div>
<div id="kernel-search" class="section level2">
<h2>Kernel Search</h2>
<p>We achived an accuracy of 0.98 in 300 epochs (<strong>Fig. 1</strong>). This single value is still hardly an indicator for the generalization potential of the CNN because we did not use an independent validation dataset to evaluate the performance of the model on unseen data. Before evaluating the generalization performance, we analyze how the CNN reacts to different kernel sizes as well as to specific data transformations. These are normalization, Savitzkiy-Golay filter, and the first and second derivative of the different representations. We then apply a loop to calculate all the different combinations. Note, we apply the <code>set.seed()</code> function before splitting the data. This way all the different combinations of kernels and data transformations actually trains and evaluates on the exact same dataset. This would not be valid if the generalization potential of the model was going to be assessed. But since we are interested in the performance of different kernel sizes and data transformation techniques, it is actually beneficial for comparison if the different models train on the same data. It would otherwise not be possible to account variations in performance, either to the kernel size, or to a different split in the training and validation sets.</p>
<pre class="r"><code>kernels = c(10,20,30,40,50,60,70,80,90,100,125,150,175,200)
types = c(&quot;raw&quot;,&quot;norm&quot;,&quot;sg&quot;,&quot;sg.norm&quot;,&quot;raw.d1&quot;, &quot;sg.norm.d1&quot;, &quot;sg.norm.d2&quot;,
          &quot;raw.d1&quot;,&quot;raw.d2&quot;,&quot;norm.d1&quot;,&quot;norm.d2&quot;)
results = data.frame(types = rep(0, length(kernels) * length(types)),
                     kernel =rep(0, length(kernels) * length(types)),
                     loss = rep(0, length(kernels) * length(types)),
                     acc = rep(0, length(kernels) * length(types)),
                     val_loss=rep(0, length(kernels) * length(types)),
                     val_acc=rep(0, length(kernels) * length(types)))

variables = ncol(data)-1
counter = 1

for (type in types){
    if (type == &quot;raw&quot;){
      tmp = data
      variables = ncol(data)-1
    }else{
      tmp = preprocess(data[ ,1:ncol(data)-1], type = type)
      variables = ncol(tmp)
      tmp$class =data$class
    }
  for (kernel in kernels){

    # splitting between training and test
    set.seed(42)
    index = caret::createDataPartition(y=tmp$class,p=.5)
    training = tmp[index$Resample1,]
    validation = tmp[-index$Resample1,]

    # splitting predictors and labels
    x_train = training[,1:variables]
    y_train = training[,1+variables]
    x_test = validation[,1:variables]
    y_test = validation[,1+variables]

    #number of preditors and unique targets
    nOutcome = length(levels(y_train))

    # function to get keras array for dataframes
    K &lt;- keras::backend()
    df_to_karray &lt;- function(df){
      d = as.matrix(df)
      d = K$expand_dims(d, axis = 2L)
      d = K$eval(d)
    }

    # coerce data to keras structure
    x_train = df_to_karray(x_train)
    x_test = df_to_karray(x_test)
    y_train = keras::to_categorical(as.numeric(y_train)-1,nOutcome)
    y_test = keras::to_categorical(as.numeric(y_test)-1,nOutcome)
    # contstruction of &quot;large&quot; neural network
    model = prepNNET(kernel, variables, nOutcome)
    history = keras::fit(model, x = x_train, y = y_train,
                          epochs=200, validation_data = list(x_test,y_test),
                          #callbacks =  callback_tensorboard(paste0(output,&quot;nnet/logs&quot;)),
                          batch_size = 10 )
    results$types[counter] = type
    results$kernel[counter] = kernel
    results$loss[counter] = history$metrics$loss[100]
    results$acc[counter] = history$metrics$acc[100]
    results$val_loss[counter] = history$metrics$val_loss[100]
    results$val_acc[counter] = history$metrics$val_acc[100]
    write.csv(results, file = paste0(output,&quot;nnet/kernels.csv&quot;))
    counter = counter + 1
  }

  print(results)
}</code></pre>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>We can now plot the results with increasing kernel sizes on the x-axis, accuracy values for the validation dataset on the y-axis and different lines for the data transformations (<strong>Fig. 2</strong>).</p>
<div class="figure" style="text-align: center">
<div id="htmlwidget-cd7de5f726db8bf4a847" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-cd7de5f726db8bf4a847">{"x":{"data":[{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.699999988079071,0.828571438789368,0.800000011920929,0.800000011920929,0.800000011920929,0.842857122421265,0.842857122421265,0.785714268684387,0.77142858505249,0.814285695552826,0.800000011920929,0.714285731315613,0.871428549289703,0.742857158184052],"text":["types: norm<br />types: norm<br />kernel:  10<br />val_acc: 0.7000000","types: norm<br />types: norm<br />kernel:  20<br />val_acc: 0.8285714","types: norm<br />types: norm<br />kernel:  30<br />val_acc: 0.8000000","types: norm<br />types: norm<br />kernel:  40<br />val_acc: 0.8000000","types: norm<br />types: norm<br />kernel:  50<br />val_acc: 0.8000000","types: norm<br />types: norm<br />kernel:  60<br />val_acc: 0.8428571","types: norm<br />types: norm<br />kernel:  70<br />val_acc: 0.8428571","types: norm<br />types: norm<br />kernel:  80<br />val_acc: 0.7857143","types: norm<br />types: norm<br />kernel:  90<br />val_acc: 0.7714286","types: norm<br />types: norm<br />kernel: 100<br />val_acc: 0.8142857","types: norm<br />types: norm<br />kernel: 125<br />val_acc: 0.8000000","types: norm<br />types: norm<br />kernel: 150<br />val_acc: 0.7142857","types: norm<br />types: norm<br />kernel: 175<br />val_acc: 0.8714285","types: norm<br />types: norm<br />kernel: 200<br />val_acc: 0.7428572"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(248,118,109,1)","dash":"solid"},"hoveron":"points","name":"norm","legendgroup":"norm","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.757142841815948,0.842857122421265,0.842857122421265,0.871428549289703,0.828571438789368,0.842857122421265,0.857142865657806,0.828571438789368,0.857142865657806,0.800000011920929,0.814285695552826,0.899999976158142,0.800000011920929,0.800000011920929],"text":["types: norm.d1<br />types: norm.d1<br />kernel:  10<br />val_acc: 0.7571428","types: norm.d1<br />types: norm.d1<br />kernel:  20<br />val_acc: 0.8428571","types: norm.d1<br />types: norm.d1<br />kernel:  30<br />val_acc: 0.8428571","types: norm.d1<br />types: norm.d1<br />kernel:  40<br />val_acc: 0.8714285","types: norm.d1<br />types: norm.d1<br />kernel:  50<br />val_acc: 0.8285714","types: norm.d1<br />types: norm.d1<br />kernel:  60<br />val_acc: 0.8428571","types: norm.d1<br />types: norm.d1<br />kernel:  70<br />val_acc: 0.8571429","types: norm.d1<br />types: norm.d1<br />kernel:  80<br />val_acc: 0.8285714","types: norm.d1<br />types: norm.d1<br />kernel:  90<br />val_acc: 0.8571429","types: norm.d1<br />types: norm.d1<br />kernel: 100<br />val_acc: 0.8000000","types: norm.d1<br />types: norm.d1<br />kernel: 125<br />val_acc: 0.8142857","types: norm.d1<br />types: norm.d1<br />kernel: 150<br />val_acc: 0.9000000","types: norm.d1<br />types: norm.d1<br />kernel: 175<br />val_acc: 0.8000000","types: norm.d1<br />types: norm.d1<br />kernel: 200<br />val_acc: 0.8000000"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(222,140,0,1)","dash":"solid"},"hoveron":"points","name":"norm.d1","legendgroup":"norm.d1","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.742857158184052,0.785714268684387,0.77142858505249,0.842857122421265,0.814285695552826,0.857142865657806,0.800000011920929,0.885714292526245,0.914285719394684,0.842857122421265,0.828571438789368,0.885714292526245,0.72857141494751,0.77142858505249],"text":["types: norm.d2<br />types: norm.d2<br />kernel:  10<br />val_acc: 0.7428572","types: norm.d2<br />types: norm.d2<br />kernel:  20<br />val_acc: 0.7857143","types: norm.d2<br />types: norm.d2<br />kernel:  30<br />val_acc: 0.7714286","types: norm.d2<br />types: norm.d2<br />kernel:  40<br />val_acc: 0.8428571","types: norm.d2<br />types: norm.d2<br />kernel:  50<br />val_acc: 0.8142857","types: norm.d2<br />types: norm.d2<br />kernel:  60<br />val_acc: 0.8571429","types: norm.d2<br />types: norm.d2<br />kernel:  70<br />val_acc: 0.8000000","types: norm.d2<br />types: norm.d2<br />kernel:  80<br />val_acc: 0.8857143","types: norm.d2<br />types: norm.d2<br />kernel:  90<br />val_acc: 0.9142857","types: norm.d2<br />types: norm.d2<br />kernel: 100<br />val_acc: 0.8428571","types: norm.d2<br />types: norm.d2<br />kernel: 125<br />val_acc: 0.8285714","types: norm.d2<br />types: norm.d2<br />kernel: 150<br />val_acc: 0.8857143","types: norm.d2<br />types: norm.d2<br />kernel: 175<br />val_acc: 0.7285714","types: norm.d2<br />types: norm.d2<br />kernel: 200<br />val_acc: 0.7714286"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(183,159,0,1)","dash":"solid"},"hoveron":"points","name":"norm.d2","legendgroup":"norm.d2","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.72857141494751,0.785714268684387,0.828571438789368,0.77142858505249,0.800000011920929,0.814285695552826,0.800000011920929,0.814285695552826,0.757142841815948,0.77142858505249,0.714285731315613,0.842857122421265,0.77142858505249,0.785714268684387],"text":["types: raw<br />types: raw<br />kernel:  10<br />val_acc: 0.7285714","types: raw<br />types: raw<br />kernel:  20<br />val_acc: 0.7857143","types: raw<br />types: raw<br />kernel:  30<br />val_acc: 0.8285714","types: raw<br />types: raw<br />kernel:  40<br />val_acc: 0.7714286","types: raw<br />types: raw<br />kernel:  50<br />val_acc: 0.8000000","types: raw<br />types: raw<br />kernel:  60<br />val_acc: 0.8142857","types: raw<br />types: raw<br />kernel:  70<br />val_acc: 0.8000000","types: raw<br />types: raw<br />kernel:  80<br />val_acc: 0.8142857","types: raw<br />types: raw<br />kernel:  90<br />val_acc: 0.7571428","types: raw<br />types: raw<br />kernel: 100<br />val_acc: 0.7714286","types: raw<br />types: raw<br />kernel: 125<br />val_acc: 0.7142857","types: raw<br />types: raw<br />kernel: 150<br />val_acc: 0.8428571","types: raw<br />types: raw<br />kernel: 175<br />val_acc: 0.7714286","types: raw<br />types: raw<br />kernel: 200<br />val_acc: 0.7857143"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(124,174,0,1)","dash":"solid"},"hoveron":"points","name":"raw","legendgroup":"raw","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.842857122421265,0.828571438789368,0.842857122421265,0.742857158184052,0.828571438789368,0.871428549289703,0.785714268684387,0.814285695552826,0.885714292526245,0.814285695552826,0.77142858505249,0.814285695552826,0.814285695552826,0.800000011920929],"text":["types: raw.d1<br />types: raw.d1<br />kernel:  10<br />val_acc: 0.8428571","types: raw.d1<br />types: raw.d1<br />kernel:  20<br />val_acc: 0.8285714","types: raw.d1<br />types: raw.d1<br />kernel:  30<br />val_acc: 0.8428571","types: raw.d1<br />types: raw.d1<br />kernel:  40<br />val_acc: 0.7428572","types: raw.d1<br />types: raw.d1<br />kernel:  50<br />val_acc: 0.8285714","types: raw.d1<br />types: raw.d1<br />kernel:  60<br />val_acc: 0.8714285","types: raw.d1<br />types: raw.d1<br />kernel:  70<br />val_acc: 0.7857143","types: raw.d1<br />types: raw.d1<br />kernel:  80<br />val_acc: 0.8142857","types: raw.d1<br />types: raw.d1<br />kernel:  90<br />val_acc: 0.8857143","types: raw.d1<br />types: raw.d1<br />kernel: 100<br />val_acc: 0.8142857","types: raw.d1<br />types: raw.d1<br />kernel: 125<br />val_acc: 0.7714286","types: raw.d1<br />types: raw.d1<br />kernel: 150<br />val_acc: 0.8142857","types: raw.d1<br />types: raw.d1<br />kernel: 175<br />val_acc: 0.8142857","types: raw.d1<br />types: raw.d1<br />kernel: 200<br />val_acc: 0.8000000"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(0,186,56,1)","dash":"solid"},"hoveron":"points","name":"raw.d1","legendgroup":"raw.d1","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.77142858505249,0.828571438789368,0.871428549289703,0.857142865657806,0.871428549289703,0.814285695552826,0.800000011920929,0.828571438789368,0.899999976158142,0.800000011920929,0.800000011920929,0.785714268684387,0.842857122421265,0.842857122421265],"text":["types: raw.d2<br />types: raw.d2<br />kernel:  10<br />val_acc: 0.7714286","types: raw.d2<br />types: raw.d2<br />kernel:  20<br />val_acc: 0.8285714","types: raw.d2<br />types: raw.d2<br />kernel:  30<br />val_acc: 0.8714285","types: raw.d2<br />types: raw.d2<br />kernel:  40<br />val_acc: 0.8571429","types: raw.d2<br />types: raw.d2<br />kernel:  50<br />val_acc: 0.8714285","types: raw.d2<br />types: raw.d2<br />kernel:  60<br />val_acc: 0.8142857","types: raw.d2<br />types: raw.d2<br />kernel:  70<br />val_acc: 0.8000000","types: raw.d2<br />types: raw.d2<br />kernel:  80<br />val_acc: 0.8285714","types: raw.d2<br />types: raw.d2<br />kernel:  90<br />val_acc: 0.9000000","types: raw.d2<br />types: raw.d2<br />kernel: 100<br />val_acc: 0.8000000","types: raw.d2<br />types: raw.d2<br />kernel: 125<br />val_acc: 0.8000000","types: raw.d2<br />types: raw.d2<br />kernel: 150<br />val_acc: 0.7857143","types: raw.d2<br />types: raw.d2<br />kernel: 175<br />val_acc: 0.8428571","types: raw.d2<br />types: raw.d2<br />kernel: 200<br />val_acc: 0.8428571"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(0,192,139,1)","dash":"solid"},"hoveron":"points","name":"raw.d2","legendgroup":"raw.d2","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.842857122421265,0.842857122421265,0.842857122421265,0.757142841815948,0.842857122421265,0.785714268684387,0.899999976158142,0.828571438789368,0.757142841815948,0.785714268684387,0.828571438789368,0.742857158184052,0.714285731315613,0.742857158184052],"text":["types: sg<br />types: sg<br />kernel:  10<br />val_acc: 0.8428571","types: sg<br />types: sg<br />kernel:  20<br />val_acc: 0.8428571","types: sg<br />types: sg<br />kernel:  30<br />val_acc: 0.8428571","types: sg<br />types: sg<br />kernel:  40<br />val_acc: 0.7571428","types: sg<br />types: sg<br />kernel:  50<br />val_acc: 0.8428571","types: sg<br />types: sg<br />kernel:  60<br />val_acc: 0.7857143","types: sg<br />types: sg<br />kernel:  70<br />val_acc: 0.9000000","types: sg<br />types: sg<br />kernel:  80<br />val_acc: 0.8285714","types: sg<br />types: sg<br />kernel:  90<br />val_acc: 0.7571428","types: sg<br />types: sg<br />kernel: 100<br />val_acc: 0.7857143","types: sg<br />types: sg<br />kernel: 125<br />val_acc: 0.8285714","types: sg<br />types: sg<br />kernel: 150<br />val_acc: 0.7428572","types: sg<br />types: sg<br />kernel: 175<br />val_acc: 0.7142857","types: sg<br />types: sg<br />kernel: 200<br />val_acc: 0.7428572"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(0,191,196,1)","dash":"solid"},"hoveron":"points","name":"sg","legendgroup":"sg","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.714285731315613,0.77142858505249,0.800000011920929,0.800000011920929,0.77142858505249,0.757142841815948,0.72857141494751,0.714285731315613,0.671428561210632,0.785714268684387,0.757142841815948,0.742857158184052,0.628571450710297,0.5],"text":["types: sg.d1<br />types: sg.d1<br />kernel:  10<br />val_acc: 0.7142857","types: sg.d1<br />types: sg.d1<br />kernel:  20<br />val_acc: 0.7714286","types: sg.d1<br />types: sg.d1<br />kernel:  30<br />val_acc: 0.8000000","types: sg.d1<br />types: sg.d1<br />kernel:  40<br />val_acc: 0.8000000","types: sg.d1<br />types: sg.d1<br />kernel:  50<br />val_acc: 0.7714286","types: sg.d1<br />types: sg.d1<br />kernel:  60<br />val_acc: 0.7571428","types: sg.d1<br />types: sg.d1<br />kernel:  70<br />val_acc: 0.7285714","types: sg.d1<br />types: sg.d1<br />kernel:  80<br />val_acc: 0.7142857","types: sg.d1<br />types: sg.d1<br />kernel:  90<br />val_acc: 0.6714286","types: sg.d1<br />types: sg.d1<br />kernel: 100<br />val_acc: 0.7857143","types: sg.d1<br />types: sg.d1<br />kernel: 125<br />val_acc: 0.7571428","types: sg.d1<br />types: sg.d1<br />kernel: 150<br />val_acc: 0.7428572","types: sg.d1<br />types: sg.d1<br />kernel: 175<br />val_acc: 0.6285715","types: sg.d1<br />types: sg.d1<br />kernel: 200<br />val_acc: 0.5000000"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(0,180,240,1)","dash":"solid"},"hoveron":"points","name":"sg.d1","legendgroup":"sg.d1","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.185714289546013,0.400000005960464,0.185714289546013,0.185714289546013,0.72857141494751,0.185714289546013,0.342857152223587,0.185714289546013,0.571428596973419,0.185714289546013,0.185714289546013,0.185714289546013,0.185714289546013,0.5],"text":["types: sg.d2<br />types: sg.d2<br />kernel:  10<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel:  20<br />val_acc: 0.4000000","types: sg.d2<br />types: sg.d2<br />kernel:  30<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel:  40<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel:  50<br />val_acc: 0.7285714","types: sg.d2<br />types: sg.d2<br />kernel:  60<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel:  70<br />val_acc: 0.3428572","types: sg.d2<br />types: sg.d2<br />kernel:  80<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel:  90<br />val_acc: 0.5714286","types: sg.d2<br />types: sg.d2<br />kernel: 100<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel: 125<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel: 150<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel: 175<br />val_acc: 0.1857143","types: sg.d2<br />types: sg.d2<br />kernel: 200<br />val_acc: 0.5000000"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(97,156,255,1)","dash":"solid"},"hoveron":"points","name":"sg.d2","legendgroup":"sg.d2","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.699999988079071,0.800000011920929,0.7714286,0.842857122421265,0.77142858505249,0.800000011920929,0.814285695552826,0.785714268684387,0.828571438789368,0.842857122421265,0.757142841815948,0.685714304447174,0.72857141494751,0.685714304447174],"text":["types: sg.norm<br />types: sg.norm<br />kernel:  10<br />val_acc: 0.7000000","types: sg.norm<br />types: sg.norm<br />kernel:  20<br />val_acc: 0.8000000","types: sg.norm<br />types: sg.norm<br />kernel:  30<br />val_acc: 0.7714286","types: sg.norm<br />types: sg.norm<br />kernel:  40<br />val_acc: 0.8428571","types: sg.norm<br />types: sg.norm<br />kernel:  50<br />val_acc: 0.7714286","types: sg.norm<br />types: sg.norm<br />kernel:  60<br />val_acc: 0.8000000","types: sg.norm<br />types: sg.norm<br />kernel:  70<br />val_acc: 0.8142857","types: sg.norm<br />types: sg.norm<br />kernel:  80<br />val_acc: 0.7857143","types: sg.norm<br />types: sg.norm<br />kernel:  90<br />val_acc: 0.8285714","types: sg.norm<br />types: sg.norm<br />kernel: 100<br />val_acc: 0.8428571","types: sg.norm<br />types: sg.norm<br />kernel: 125<br />val_acc: 0.7571428","types: sg.norm<br />types: sg.norm<br />kernel: 150<br />val_acc: 0.6857143","types: sg.norm<br />types: sg.norm<br />kernel: 175<br />val_acc: 0.7285714","types: sg.norm<br />types: sg.norm<br />kernel: 200<br />val_acc: 0.6857143"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(199,124,255,1)","dash":"solid"},"hoveron":"points","name":"sg.norm","legendgroup":"sg.norm","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.742857158184052,0.77142858505249,0.885714292526245,0.871428549289703,0.871428549289703,0.885714292526245,0.828571438789368,0.857142865657806,0.857142865657806,0.77142858505249,0.77142858505249,0.871428549289703,0.842857122421265,0.757142841815948],"text":["types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  10<br />val_acc: 0.7428572","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  20<br />val_acc: 0.7714286","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  30<br />val_acc: 0.8857143","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  40<br />val_acc: 0.8714285","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  50<br />val_acc: 0.8714285","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  60<br />val_acc: 0.8857143","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  70<br />val_acc: 0.8285714","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  80<br />val_acc: 0.8571429","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel:  90<br />val_acc: 0.8571429","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel: 100<br />val_acc: 0.7714286","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel: 125<br />val_acc: 0.7714286","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel: 150<br />val_acc: 0.8714285","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel: 175<br />val_acc: 0.8428571","types: sg.norm.d1<br />types: sg.norm.d1<br />kernel: 200<br />val_acc: 0.7571428"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(245,100,227,1)","dash":"solid"},"hoveron":"points","name":"sg.norm.d1","legendgroup":"sg.norm.d1","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,20,30,40,50,60,70,80,90,100,125,150,175,200],"y":[0.557142853736877,0.699999988079071,0.785714268684387,0.671428561210632,0.757142841815948,0.699999988079071,0.571428596973419,0.685714304447174,0.757142841815948,0.785714268684387,0.77142858505249,0.72857141494751,0.77142858505249,0.72857141494751],"text":["types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  10<br />val_acc: 0.5571429","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  20<br />val_acc: 0.7000000","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  30<br />val_acc: 0.7857143","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  40<br />val_acc: 0.6714286","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  50<br />val_acc: 0.7571428","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  60<br />val_acc: 0.7000000","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  70<br />val_acc: 0.5714286","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  80<br />val_acc: 0.6857143","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel:  90<br />val_acc: 0.7571428","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel: 100<br />val_acc: 0.7857143","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel: 125<br />val_acc: 0.7714286","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel: 150<br />val_acc: 0.7285714","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel: 175<br />val_acc: 0.7714286","types: sg.norm.d2<br />types: sg.norm.d2<br />kernel: 200<br />val_acc: 0.7285714"],"type":"scatter","mode":"lines","line":{"width":5.66929133858268,"color":"rgba(255,100,176,1)","dash":"solid"},"hoveron":"points","name":"sg.norm.d2","legendgroup":"sg.norm.d2","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":43.1050228310502},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.5,209.5],"tickmode":"array","ticktext":["50","100","150","200"],"tickvals":[50,100,150,200],"categoryorder":"array","categoryarray":["50","100","150","200"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"kernel size","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.149285718053579,0.950714290887117],"tickmode":"array","ticktext":["0.2","0.4","0.6","0.8"],"tickvals":[0.2,0.4,0.6,0.8],"categoryorder":"array","categoryarray":["0.2","0.4","0.6","0.8"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"validation accuracy","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.96751968503937},"annotations":[{"text":"types","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"1b5b178865d9":{"colour":{},"x":{},"y":{},"type":"scatter"}},"cur_data":"1b5b178865d9","visdat":{"1b5b178865d9":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
<strong>Fig. 2</strong>: Accuracy results for different kernel sizes.
</p>
</div>
<p>We can observe that the pattern of accuracy is highly variable dependent on the kernel size as well as between different data transformations. For example, the use of the second derivative of the Savitzkiy-Golay filtered data yields to very low accuracies across all kernel sizes. To aid the selection of an appropriate kernel size and data transformation we calculated some descriptive statistic values to find optimal configurations. One indicator are the kernel sizes which deliver the highest accuracy results on average. Another indicator for optimal configurations might be the highest accurcies achieved in absolute terms.</p>
<pre class="r"><code>kernelAcc = aggregate(val_acc ~ kernel, results, mean)
kernelAcc = kernelAcc[order(-kernelAcc$val_acc), ]

type = results[which(results$kernel == kernelAcc$kernel[1]),]
type = type[order(-type$val_acc), ]

highest = results[order(-results$val_acc),]</code></pre>
<p>On average, a kernel size of 50 delivered the highest accuracy of 0.81 (<strong>Tab. 1</strong>). A kernel size of 90 yielded to the second-highest accuracy.</p>
<table>
<caption><strong>Tab. 1</strong>: Average performance of kernel size across data preprocessing types.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">kernel</th>
<th align="right">val_acc</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5</td>
<td align="right">50</td>
<td align="right">0.8071429</td>
</tr>
<tr class="even">
<td>9</td>
<td align="right">90</td>
<td align="right">0.7940476</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="right">30</td>
<td align="right">0.7690476</td>
</tr>
<tr class="even">
<td>2</td>
<td align="right">20</td>
<td align="right">0.7654762</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="right">60</td>
<td align="right">0.7630952</td>
</tr>
<tr class="even">
<td>7</td>
<td align="right">70</td>
<td align="right">0.7559524</td>
</tr>
<tr class="odd">
<td>8</td>
<td align="right">80</td>
<td align="right">0.7511905</td>
</tr>
<tr class="even">
<td>4</td>
<td align="right">40</td>
<td align="right">0.7511905</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="right">100</td>
<td align="right">0.7500000</td>
</tr>
<tr class="even">
<td>12</td>
<td align="right">150</td>
<td align="right">0.7416667</td>
</tr>
<tr class="odd">
<td>11</td>
<td align="right">125</td>
<td align="right">0.7333333</td>
</tr>
<tr class="even">
<td>13</td>
<td align="right">175</td>
<td align="right">0.7250000</td>
</tr>
<tr class="odd">
<td>14</td>
<td align="right">200</td>
<td align="right">0.7214286</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">10</td>
<td align="right">0.6904762</td>
</tr>
</tbody>
</table>
<p>When we order the results according to the absolute accuracies achieved, it can be observed that there are only four pre-processing types and kernel sizes which yielded to an accuracy of 0.9 or higher (<strong>Tab. 2</strong>). The second derivative of the normalized data yielded to an accuracy of 0.91 at a kernel size of 90. The simple Savitzkiy-Golay smoothed data yielded to an accuracy of 0.9 at a kernel size of 70. The second derivative of the raw data yielded to an accuracy of 0.9 at a kernel size of 90. The first derivative of the normalised data yielded to an accuracy of 0.9 at a kernel size of 150.</p>
<table>
<caption><strong>Tab. 2</strong>: The ten highest accuracy results for different preprocessing types at varying kernel sizes.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">X</th>
<th align="left">types</th>
<th align="right">kernel</th>
<th align="right">loss</th>
<th align="right">acc</th>
<th align="right">val_loss</th>
<th align="right">val_acc</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>163</td>
<td align="right">163</td>
<td align="left">norm.d2</td>
<td align="right">90</td>
<td align="right">0.2898488</td>
<td align="right">0.8961039</td>
<td align="right">0.8323456</td>
<td align="right">0.9142857</td>
</tr>
<tr class="even">
<td>35</td>
<td align="right">35</td>
<td align="left">sg</td>
<td align="right">70</td>
<td align="right">0.2537513</td>
<td align="right">0.9350649</td>
<td align="right">1.3861195</td>
<td align="right">0.9000000</td>
</tr>
<tr class="odd">
<td>135</td>
<td align="right">135</td>
<td align="left">raw.d2</td>
<td align="right">90</td>
<td align="right">0.1529336</td>
<td align="right">0.9350649</td>
<td align="right">1.5874773</td>
<td align="right">0.9000000</td>
</tr>
<tr class="even">
<td>152</td>
<td align="right">152</td>
<td align="left">norm.d1</td>
<td align="right">150</td>
<td align="right">0.1060385</td>
<td align="right">0.9740260</td>
<td align="right">0.8856056</td>
<td align="right">0.9000000</td>
</tr>
<tr class="odd">
<td>65</td>
<td align="right">65</td>
<td align="left">raw.d1</td>
<td align="right">90</td>
<td align="right">0.1509757</td>
<td align="right">0.9350649</td>
<td align="right">1.0225650</td>
<td align="right">0.8857143</td>
</tr>
<tr class="even">
<td>101</td>
<td align="right">101</td>
<td align="left">sg.norm.d1</td>
<td align="right">30</td>
<td align="right">0.1817555</td>
<td align="right">0.9480519</td>
<td align="right">0.4789599</td>
<td align="right">0.8857143</td>
</tr>
<tr class="odd">
<td>104</td>
<td align="right">104</td>
<td align="left">sg.norm.d1</td>
<td align="right">60</td>
<td align="right">0.2074841</td>
<td align="right">0.9350649</td>
<td align="right">0.6712436</td>
<td align="right">0.8857143</td>
</tr>
<tr class="even">
<td>162</td>
<td align="right">162</td>
<td align="left">norm.d2</td>
<td align="right">80</td>
<td align="right">0.0823583</td>
<td align="right">0.9610389</td>
<td align="right">0.5753851</td>
<td align="right">0.8857143</td>
</tr>
<tr class="odd">
<td>166</td>
<td align="right">166</td>
<td align="left">norm.d2</td>
<td align="right">150</td>
<td align="right">0.0143513</td>
<td align="right">1.0000000</td>
<td align="right">1.1288143</td>
<td align="right">0.8857143</td>
</tr>
<tr class="even">
<td>27</td>
<td align="right">27</td>
<td align="left">norm</td>
<td align="right">175</td>
<td align="right">0.0565004</td>
<td align="right">0.9870130</td>
<td align="right">1.4892539</td>
<td align="right">0.8714285</td>
</tr>
</tbody>
</table>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross Validation</h2>
<p>After finding the optimal kernel sizes for different pre-processing techniques, a cross-validation approach was used to find the configuration with the optimal generalization potential. The documentation of the results can be found <a href="cnn_crossvalidation.html">here</a>.</p>
</div>
<div id="citations-on-this-page" class="section level2">
<h2>Citations on this page</h2>
<div id="refs">
<div id="ref-Berisha2019">
<p>Berisha, Sebastian, Mahsa Lotfollahi, Jahandar Jahanipour, Ilker Gurcan, Michael Walsh, Rohit Bhargava, Hien Van Nguyen, and David Mayerich. 2019. “Deep learning for FTIR histology: leveraging spatial and spectral features with convolutional neural networks.” <em>Analyst</em> 144 (5). Royal Society of Chemistry: 1642–53. <a href="https://doi.org/10.1039/c8an01495g">https://doi.org/10.1039/c8an01495g</a>.</p>
</div>
<div id="ref-Ghosh2019">
<p>Ghosh, Kunal, Annika Stuke, Milica Todorovic, Peter Bjørn Jørgensen, Mikkel N Schmidt, Aki Vehtari, Patrick Rinke, et al. 2019. “FULL PAPER 1801367 (1 of 7) Deep Learning Spectroscopy: Neural Networks for Molecular Excitation Spectra.” <a href="https://doi.org/10.1002/advs.201801367">https://doi.org/10.1002/advs.201801367</a>.</p>
</div>
<div id="ref-IsmailFawaz2019">
<p>Ismail Fawaz, Hassan, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre Alain Muller. 2019. “Deep learning for time series classification: a review.” <em>Data Mining and Knowledge Discovery</em> 33 (4): 917–63. <a href="https://doi.org/10.1007/s10618-019-00619-1">https://doi.org/10.1007/s10618-019-00619-1</a>.</p>
</div>
<div id="ref-Liu2017">
<p>Liu, Jinchao, Margarita Osadchy, Lorna Ashton, Michael Foster, Christopher J. Solomon, and Stuart J. Gibson. 2017. “Deep convolutional neural networks for Raman spectrum recognition: A unified solution.” <em>Analyst</em> 142 (21): 4067–74. <a href="https://doi.org/10.1039/c7an01371j">https://doi.org/10.1039/c7an01371j</a>.</p>
</div>
<div id="ref-Rawat2017">
<p>Rawat, Waseem, and Zenghui Wang. 2017. “Deep convolutional neural networks for image classification: A comprehensive review.” <em>Neural Computation</em> 29 (9): 2352–2449. <a href="https://doi.org/10.1162/NECO_a_00990">https://doi.org/10.1162/NECO_a_00990</a>.</p>
</div>
</div>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.6.1 (2019-07-05)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.1

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] plotly_4.9.0              tensorflow_1.14.0        
 [3] abind_1.4-5               e1071_1.7-2              
 [5] keras_2.2.4.1             workflowr_1.4.0.9001     
 [7] baseline_1.2-1            gridExtra_2.3            
 [9] stringr_1.4.0             prospectr_0.1.3          
[11] RcppArmadillo_0.9.600.4.0 openxlsx_4.1.0.1         
[13] magrittr_1.5              ggplot2_3.2.0            
[15] reshape2_1.4.3            dplyr_0.8.3              

loaded via a namespace (and not attached):
 [1] httr_1.4.1        tidyr_0.8.3       jsonlite_1.6     
 [4] viridisLite_0.3.0 foreach_1.4.7     shiny_1.3.2      
 [7] assertthat_0.2.1  highr_0.8         yaml_2.2.0       
[10] pillar_1.4.2      backports_1.1.4   lattice_0.20-38  
[13] glue_1.3.1        reticulate_1.13   digest_0.6.20    
[16] promises_1.0.1    colorspace_1.4-1  htmltools_0.3.6  
[19] httpuv_1.5.1      Matrix_1.2-17     plyr_1.8.4       
[22] pkgconfig_2.0.2   SparseM_1.77      purrr_0.3.2      
[25] xtable_1.8-4      scales_1.0.0      whisker_0.3-2    
[28] later_0.8.0       git2r_0.26.1      tibble_2.1.3     
[31] generics_0.0.2    withr_2.1.2       lazyeval_0.2.2   
[34] crayon_1.3.4      mime_0.7          evaluate_0.14    
[37] fs_1.3.1          class_7.3-15      tools_3.6.1      
[40] data.table_1.12.2 munsell_0.5.0     zip_2.0.3        
[43] compiler_3.6.1    rlang_0.4.0       grid_3.6.1       
[46] iterators_1.0.12  htmlwidgets_1.3   crosstalk_1.0.0  
[49] base64enc_0.1-3   labeling_0.3      rmarkdown_1.14   
[52] gtable_0.3.0      codetools_0.2-16  R6_2.4.0         
[55] tfruns_1.4        knitr_1.24        zeallot_0.1.0    
[58] rprojroot_1.3-2   stringi_1.4.3     Rcpp_1.0.2       
[61] tidyselect_0.2.5  xfun_0.8         </code></pre>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
