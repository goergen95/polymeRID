---
title: "Preparation"
author: Darius Goergen
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
bibliography: library.bib
link-citations: yes
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("code/setup_website.R")
source("code/functions.R")
```

Random Forest (RF) is a machine learning algorithm which is based on the 
traditional concept of decision trees. It's populat implementation was
developed by @Breiman2001. It represents an ensemble classifier which has been 
reported to be the primary choice between different ensemble classifiers due to 
its easy handling and high classification accuracies [@Sagi2018]. 
**More elaborations on scientific background**


Different levels of data preprocessing might emphasize different features of 
the patterns to be learned by an algorithm. To grasp this, different transformations
of the data were presented to the RF algorithm. Additionally, the raw data signal
was jittered to test which transformation might proove beneficial in delivering 
high classification accuracies even in the presence of noise. To test this, 
we define a function which adds noise to the raw data and returns a list with
the number of elements equal to the levels of noise applied.

```{r}
addNoise = function(data, levels = c(0), category="class"){
  data.return = list()
  index = which(names(data) == category)
  for (n in levels){
    tmp = as.matrix(data[ , -index])
    if (n == 0){
      tmp = data
    }else{
      tmp = as.data.frame(jitter(tmp, n))
      tmp[category] = data[category]
    }
    data.return[[paste("noise", n, sep="")]] = tmp
  }
  return(data.return)
}

data = read.csv(file = paste0(ref, "reference_database.csv"), header = TRUE)
noisy_data = addNoise(data, levels = c(0,10,100,250,500), category = "class")

# indivitual elements can be selected by using [[ and refering to the index or the name
head(noisy_data[["noise100"]])[1:3,1:3]
```

Then, in another user defined function which uses the `noisy_data` objected as input
specified data transormations are applied. These are normalization which centers
and scales the input data, as well as different forms of the Savitkiy-Golay filter [@Savitzky1964]
and first and second derivative of a raw spectrum. The functions iterates through
the noise level elements in the `noisy_data` object and returns each specified 
transformation in a list element below the noise level. The exemplary function below
applies the pre-processing for normalisation, standard filtering and first 
derivative only. The implementation of the function used in the project
can be found [here](https://github.com/goergen95/polymeRID/blob/master/code/functions.R#12).


```{r}
createTrainingSet = function(data, category = "class",
                             SGpara = list(p=3,w=11), lag = 15){
  
  data.return = list()
  for (noise in names(data)){
    
    tmp = as.data.frame(data[[noise]])
    classes = tmp[,category]
    tmp = tmp[!names(tmp) %in% category]
    
    # original data
    data.return[[noise]][["raw"]] = as.data.frame(data[[noise]])
    
    # normalised data
    data_norm = preprocess(tmp, type="norm")
    data_norm[category] = classes
    data.return[[noise]][["norm"]] = data_norm
    
    # SG-filtered data
    data_sg = preprocess(tmp, type="sg", SGpara = SGpara)
    data_sg[category] = classes
    data.return[[noise]][["sg"]] = data_sg
    
    # first derivative of original data
    data_rawd1 = preprocess(tmp, type="raw.d1", lag = lag)
    data_rawd1[category] = classes
    data.return[[noise]][["raw.d1"]] = data_rawd1
    
  }
  return(data.return)
}

# applying the function
test_dataset = createTrainingSet(noisy_data, category = "class")

# individual transformations at a certain noise level can be accessed with [[
head(test_dataset[["noise500"]][["raw.d1"]])[1:3,1:3]
```

The data base of [@Primpke2018] currently shows 1863 variables for each observations.
Most of these data points do not bear relevant information to distinguish between
differenty types of particles. To shorten the computation time, one can use 
dimensionality reduction techniques such as principal component analysis (PCA).
PCA also has been used to transform spectral data of microplastics in marine 
ecosystems before [@Jung2018;@Lorenzo-Navarro2018]. PCA basically takes the input
data for a given number of observation, rotates the axis 
