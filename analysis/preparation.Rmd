---
title: "Preparation"
author: Darius Goergen
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
bibliography: library.bib
link-citations: yes
runtime: shiny
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("code/setup_website.R")
source("code/functions.R")
```

## Reference Data
For this project we used a data base published by @Primpke2018 online. 
The data base can be downloaded 
[here](https://static-content.springer.com/esm/art%3A10.1007%2Fs00216-018-1156-x/MediaObjects/216_2018_1156_MOESM2_ESM.xlsx).
The authors state the samples were collected based on the FTIR-spectrometer Bruker Tensor 27 System
for the spectral range 4000 to 40 1/cm. Additionally, some data of polymer-based
fibers and spectra of biological origins were received from the [Bremer Faserinstitut](https://www.faserinstitut.de/).
During preprocessing, they applied a concave rubberband correction based on 10 
iterations and 64 baseline points. They also excluded the C02 band between
2420 to 2200 1/cm by setting the data points to 0. This should be kept in mind,
since the inclusion of additional reference samples requires the same procedure
for the data base to stay in a consistent state. The data provided by @Primpke2018 
shows a spectral resolution of 2.1 1/cm. Additional reference samples need to be
resampled to the same spectral resolution and the same baseline correction should 
be applied.

To ensure consistency, the data base was read into R and the wavenumbers were saved 
in a separate file for the future use of resampling additional reference spectra.

```{r read-in-data}
library(openxlsx)
url = "https://static-content.springer.com/esm/art%3A10.1007%2Fs00216-018-1156-x/MediaObjects/216_2018_1156_MOESM2_ESM.xlsx"

data = openxlsx::read.xlsx(url)
# extract wavenumbers from first row
wavenumbers = as.numeric(names(data)[2:1864])
# saving wavenumbers to reference sample directory
saveRDS(wavenumbers, paste0(ref, "wavenumbers.rds"))
```

An important feature of any data base is the distribution of the different classes.
Here, we only print the 20 most common classes, since there are a lot of reference samples
only found once or twice within the data base. 

```{r analyse-classes}
data$Abbreviation = as.factor(data$Abbreviation)
summary(data$Abbreviation)[1:10]

```

## Construction of the Data Base
We are interested in assigning the correct class to potential plastic particles. 
The most important classes found in the data base to us are thus the ones of 
artificial polymer origin. However, sometimes also particles of biological origin
will be subject to a spectral analysis, because they resemble the appearance of 
microplastics in environmental samples. Any machine learning algorithm trained 
only with reference samples from plastics would eventually assign a plastic-class 
also to the particles of biological origin. It will just assign the class with the
greatest similarity to the classes it learned. This can lead to so-called false 
positive errors. To reduce the error of false positives, we include some of the 
samples of biological origin as well. We summaries these samples to broader classes.

```{r retrieve-target-classes}
# furs and wools
indexFur = grep("fur", data$Abbreviation)
indexWool = grep("wool", data$Abbreviation)
furs = data[c(indexFur, indexWool), ]
furs = furs[ , c(2:1864)] # leave out index column
names(furs) = paste("wvn", wavenumbers, sep="")
furs$class = "FUR"

# fibres
indexFibre = grep("fibre", data$Abbreviation)
fibre = data[indexFibre, ]
fibre = fibre[ , c(2:1864)] # leave out index column
names(fibre) = paste("wvn", wavenumbers, sep="")
fibre$class = "FIBRE"

# wood
indexWood = grep("wood", data$Abbreviation)
wood = data[indexWood, ]
wood = wood[ , c(2:1864)] # leave out index colums
names(wood) = paste("wvn", wavenumbers, sep="")
wood$class = "WOOD"

# synthetic polymers
polyIndex = which(data$`Natural./Synthetic` =="synthetic polymer")
syntPolymer = data[polyIndex,]
counts = summary(syntPolymer$Abbreviation)
polyNames = names(counts)[1:10] # only major polymers
syntPolymer = syntPolymer[which(syntPolymer$Abbreviation %in%  polyNames) , ]
classes = droplevels(syntPolymer$Abbreviation)
syntPolymer = syntPolymer[ , c(2:1864)] # leave out index column
names(syntPolymer) = paste("wvn",wavenumbers,sep="")
syntPolymer$class = as.character(classes)

# lets group together some synthetic polymer classes
syntPolymer$class[grep("Nylon",syntPolymer$class)] = "PA"
```

## Class Distribution

We now bind the reference samples together and take a look at the distribution
of classes in the resulting data frame, which is the concrete data base for the 
following calculations.

```{r binding-data}
data = rbind(furs,wood,fibre,syntPolymer) 
data$class = as.factor(data$class)
summary(data$class)

```

In total, 93 (53%) reference samples of plastic polymers are present in the data base
and 44 (47%) of biological origin. Within the plastic samples, we find that the data 
is very balanced with no single class showing less than 7 samples. For the samples
of biological origin, however, the class `FIBRE` dominates the distribution.
This could prove as an disadvantage if a machine learning algorithm picks up 
this unbalance by minimizing its error rate simply by more frequently predicting
the `FIBRE` class. At this point, we will leave the resulting data base as it is 
and save it to disk.
We save the data in individual files as well as in a comprehensive data base in 
csv format. This way we ensure that later extensions to the data base are easy to
manage.

```{r write-data}
write.csv(data, file = paste0(ref, "reference_database.csv"), row.names=FALSE)

# writing class control file
classIndex = as.character(unique(data$class))

for (class in classIndex){
  tmp = data[data$class==class , ]
  write.csv(tmp, file = paste0(ref, "reference_", class, ".csv"), row.names=FALSE)
}

write(classIndex, paste0(ref, "classes.txt"))
```

```{r preparation-shiny-app, echo=FALSE, warning=FALSE, message=FALSE}
#source("plot_functions.R")
#wavenumbers = readRDS("wavenumbers.rds")
#data = read.csv(file = "reference_database.csv", header = TRUE)
library(shiny)
library(plotly)
shinyApp(

  ui = fluidPage(
    selectInput("class", "Class:",
                choices = unique(data$class)),
    plotOutput("meanSpectrumPlot")
  ),

  server = function(input, output) {
    output$meanSpectrumPlot = renderPlot({
      p = meanplot(data, wavenumbers = wavenumbers, class = input$class)
      p = ggplotly(p)
      p 
    })
  },

  options = list(height = 500)
)

```


## Citations on this page

