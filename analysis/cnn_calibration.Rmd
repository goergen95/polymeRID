---
title: "Convolutional Neural Network"
author: Darius Goergen
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
bibliography: library.bib
link-citations: yes
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("code/setup_website.R")
source("code/functions.R")
data = read.csv(file = paste0(ref, "reference_database.csv"), header = TRUE)
sg.data = preprocess(data[,1:ncol(data)-1], type = "sg")
sg.data$class = data$class
nOutcome = length(unique(data$class))
```

Now, we want to test for the generalization potential of a CNN trained with the 
parameters mentioned above. For this, we also perform a 10-fold cross-validation 
which we are going to repeat 5 times. 

```{r cnn-calibration-cv, eval = FALSE}

data = read.csv(file = paste0(ref, "reference_database.csv"), header = TRUE)

kernel = 70
folds = 10
repeats = 5
p = 0.5
nOutcome = length(unique(data$class))

sg.data = preprocess(data[,1:ncol(data)-1], type = "norm")
sg.data$class = data$class
# preparing data inputs
set.seed(42)
foldIndex = lapply(1:repeats, caret::createDataPartition, y=sg.data$class, times = folds, p=p)
foldIndex = do.call(c,foldIndex)

cvData = list()
for (rep in 1:repeats){
  rep_Index = foldIndex[(rep*folds-folds+1):(rep*folds)] #always jump to the correct number of folds forward for each repeat
  
  dataFold = lapply(1:folds,function(x){
    
    training = sg.data[unlist(rep_Index[x]), ]
    validation = sg.data[-unlist(rep_Index[x]), ]
    foldtmp = list(training,validation)
    names(foldtmp) = c("training","validation")
    return(foldtmp)
  })
  cvData[[rep]] = dataFold
}
results = data.frame(repeats = rep(0,repeats*folds),
                     fold = rep(0,repeats*folds),
                     loss = rep(0,repeats*folds),
                     acc = rep(0,repeats*folds))
counter = 1
for (rep in 1:repeats){
  #print(paste0("Starting repeat ",rep," out of ",repeats,"."))
  for (fold in 1:folds){
    
    variables = ncol(cvData[[rep]][[fold]][[1]])-1
    x_train = cvData[[rep]][[fold]][["training"]][,1:variables]
    y_train = unlist(cvData[[rep]][[fold]][["training"]][1+variables])
    x_test = cvData[[rep]][[fold]][["validation"]][,1:variables]
    y_test = unlist(cvData[[rep]][[fold]][["validation"]][1+variables])
    
    # function to get keras array for dataframes
    K <- keras::backend()
    df_to_karray <- function(df){
      tmp = as.matrix(df)
      tmp = K$expand_dims(tmp, axis = 2L)
      tmp = K$eval(tmp)
    }
    
    # coerce data to keras structure
    x_train = df_to_karray(x_train)
    x_test = df_to_karray(x_test)
    y_train = keras::to_categorical(as.numeric(y_train)-1,nOutcome)
    y_test = keras::to_categorical(as.numeric(y_test)-1,nOutcome)
    
    # fitting the model
    kernelMod = prepNNET(kernel, variables, nOutcome = nOutcome)
    historyMod =  keras::fit(kernelMod, x = x_train, y = y_train,
                             epochs=300,
                             batch_size = 10 )
    
    evalK = keras::evaluate(kernelMod, x=x_test, y=y_test)
    results$repeats[counter] = rep
    results$fold[counter] = fold
    results$loss[counter] = evalK$loss
    results$acc[counter] = evalK$acc
    print(results[counter,])
    counter = counter + 1
    write.csv(results, file = paste0(output,"nnet/cv/cvResults_K",kernel,".csv"))
  }
}

```
```{r cnn-calibration-readresults, echo = FALSE}
results = read.csv(paste0(output,"nnet/cv/cvResults_K70.csv"))
```

We can retrive information about the accurcies for the individual repeats and
for the whole cross-validation by calculating average values.
```{r cnn-calibration-aggregates}
repResults = aggregate(acc ~ repeats ,results,mean)
dvResult = mean(results$acc)
```
```{r cnn-calibration-kntirOutput, echo=FALSE}
knitr::kable(repResults)
cat(paste0("The overall accuracy for the CNN is ", round(dvResult,2),"."))
```
Finally, we create a CNN based on the complete data set and save it to disk
for future classifications
```{r cnn-calibration-finallModel, eval=FALSE}
K = keras::backend()
x_train = as.matrix(sg.data[,1:ncol(sg.data)-1])
x = K$expand_dims(x_train, axis = 2L)
x_train = K$eval(x)
y_train = keras::to_categorical(as.numeric(sg.data$class)-1, nOutcome)


model = prepNNET(kernel = 70, variables = ncol(sg.data)-1, nOutcome = length(unique(sg.data$class)))
history = fit(model, x = x_train, y = y_train, batch_size = 10, epochs = 300)

keras::save_model_hdf5(model, filepath = paste0(mod,"BASE/cnnModel.hdf"))
```
Again, we can look at the training history to get an idea of the training process.
```{r cnn-calibration-plothistory, echo=FALSE}
history = readRDS(paste0(output,"cnn_calibration_history.rds"))
history
p = plot(history)
p = p + theme_minimal()
plotly::ggplotly(p)
```



